{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f749b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    正弦位置编码，每一层一个独立向量，形状为 [1, num_layers, dim]。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer(\"pe\", self._build_encoding())\n",
    "\n",
    "    def _build_encoding(self):\n",
    "        position = torch.arange(self.num_layers).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.num_layers, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # [1, num_layers, d_model]\n",
    "\n",
    "    def forward(self, device):\n",
    "        return self.pe.to(device)  # [1, num_layers, d_model]\n",
    "\n",
    "class TransformerProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    使用 Transformer 架构对所有层的 LoRA 参数生成进行建模。\n",
    "    每一层作为一个 token，输入为文档嵌入加 token embedding 加位置编码。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.token_embeddings = nn.Parameter(torch.randn(num_layers, input_dim))  # 可学习 token 表示\n",
    "        self.pos_encoding = PositionalEncoding(num_layers, input_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.output_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, doc_embed):\n",
    "        # doc_embed: [1, input_dim]\n",
    "        batch_size = doc_embed.size(0)\n",
    "\n",
    "        tokens = self.token_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, D]\n",
    "        doc_expand = doc_embed.unsqueeze(1).expand(-1, self.num_layers, -1)     # [B, L, D]\n",
    "        pos_embed = self.pos_encoding(doc_embed.device).expand(batch_size, -1, -1)  # [B, L, D]\n",
    "\n",
    "        x = tokens + doc_expand + pos_embed  # [B, L, D]\n",
    "        out = self.transformer(x.transpose(0, 1)).transpose(0, 1)  # [B, L, D]\n",
    "        return self.output_layer(out)  # [B, L, output_dim]\n",
    "\n",
    "class ParameterTranslator(nn.Module):\n",
    "    \"\"\"\n",
    "    参数生成模块：输入文档 → 动态生成各层 MLP 模块的 LoRA 参数（使用 Transformer）\n",
    "    支持位置编码按位加法，模型初始化自动读取维度参数。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model: PreTrainedModel, llm_model: PreTrainedModel, \n",
    "                 lora_rank=2, projector_hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = embedding_model.config.hidden_size if hasattr(embedding_model.config, 'hidden_size') else embedding_model.config.d_model\n",
    "        self.num_layers = llm_model.config.num_hidden_layers\n",
    "        self.ff_hidden_dim = llm_model.config.intermediate_size\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        self.module_names = [\"down_proj\", \"up_proj\", \"gate_proj\"]\n",
    "\n",
    "        self.projectors = nn.ModuleList([\n",
    "            TransformerProjector(\n",
    "                num_layers=self.num_layers,\n",
    "                input_dim=self.input_dim,\n",
    "                hidden_dim=projector_hidden_dim,\n",
    "                output_dim=self._get_lora_dim(m),\n",
    "                num_heads=4\n",
    "            )\n",
    "            for m in self.module_names\n",
    "        ])\n",
    "\n",
    "    def _get_lora_dim(self, module_name):\n",
    "        return self.lora_rank * (self.input_dim + self.ff_hidden_dim)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "    def forward(self, doc_embed):\n",
    "        \"\"\"\n",
    "        输入：doc_embed [1, input_dim]\n",
    "        输出：字典形式 LoRA 参数（每层 × 每模块）\n",
    "        \"\"\"\n",
    "        outputs = defaultdict(list)\n",
    "        device = doc_embed.device\n",
    "\n",
    "        for i, module_name in enumerate(self.module_names):\n",
    "            lora_matrix = self.projectors[i](doc_embed)  # [1, num_layers, output_dim]\n",
    "            lora_matrix = lora_matrix.squeeze(0)  # [num_layers, output_dim]\n",
    "\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                lora_out = lora_matrix[layer_idx]  # [output_dim]\n",
    "\n",
    "                if module_name == \"down_proj\":\n",
    "                    A = lora_out[:self.lora_rank * self.ff_hidden_dim].view(self.lora_rank, self.ff_hidden_dim)\n",
    "                    B = lora_out[self.lora_rank * self.ff_hidden_dim:].view(self.input_dim, self.lora_rank)\n",
    "                else:\n",
    "                    A = lora_out[:self.lora_rank * self.input_dim].view(self.lora_rank, self.input_dim)\n",
    "                    B = lora_out[self.lora_rank * self.input_dim:].view(self.ff_hidden_dim, self.lora_rank)\n",
    "\n",
    "                key_A = f\"base_model.model.model.layers.{layer_idx}.mlp.{module_name}.lora_A.weight\"\n",
    "                key_B = f\"base_model.model.model.layers.{layer_idx}.mlp.{module_name}.lora_B.weight\"\n",
    "\n",
    "                outputs[key_A] = A\n",
    "                outputs[key_B] = B\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ced6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "# Initialize models\n",
    "embedding_model = T5EncoderModel.from_pretrained(\"./models/t5-base\",device_map=\"cpu\")\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(\"./models/t5-base\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"./models/Llama-3.2-1B-Instruct\",device_map=\"cpu\",torch_dtype=torch.bfloat16)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=2,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\"down_proj\", \"up_proj\", \"gate_proj\"],  # 指定 LoRA 注入的模块\n",
    ")\n",
    "print(llm_model)\n",
    "llm_model = get_peft_model(llm_model, peft_config)\n",
    "# embedding_model.eval()\n",
    "# llm_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cafb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterTranslator(\n",
      "  (doc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (projectors): ModuleList(\n",
      "    (0-2): 3 x TransformerProjector(\n",
      "      (pos_encoding): PositionalEncoding()\n",
      "      (transformer): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (linear1): Linear(in_features=768, out_features=512, bias=False)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=768, bias=False)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=20480, bias=False)\n",
      "        (1): LayerNorm((20480,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total parameters: 66193921\n"
     ]
    }
   ],
   "source": [
    "from train_my_dyprag import *\n",
    "\n",
    "# Initialize translator\n",
    "translator = ParameterTranslator(\n",
    "    embedding_model=embedding_model,\n",
    "    llm_model=llm_model,\n",
    "    lora_rank=2,\n",
    "    projector_hidden_dim=1024,\n",
    ")#66193921\n",
    "translator=translator.to(\"cpu\").to(torch.bfloat16)\n",
    "print(translator)\n",
    "total_params = sum(p.numel() for p in translator.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "# translator.train()\n",
    "# # Generate dummy input\n",
    "# doc_embed = torch.randn(1, translator.input_dim).to(translator.device).to(torch.bfloat16)  # [1, input_dim]\n",
    "\n",
    "# # Forward pass\n",
    "# output = translator(doc_embed)\n",
    "\n",
    "# # Show sample output\n",
    "# sample_keys = list(output.keys())[:6]\n",
    "# summary = {\n",
    "#     \"input_dim\": translator.input_dim,\n",
    "#     \"num_layers\": translator.num_layers,\n",
    "#     \"ff_hidden_dim\": translator.ff_hidden_dim,\n",
    "#     \"total_predicted_keys\": len(output),\n",
    "#     \"sample_keys\": sample_keys,\n",
    "#     \"sample_shapes\": {k: output[k].shape for k in sample_keys}\n",
    "# }\n",
    "# summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttentionParameterTranslator(\n",
      "  (doc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (projectors): ModuleList(\n",
      "    (0-2): 3 x CrossAttentionProjector(\n",
      "      (cross_attns): ModuleList(\n",
      "        (0-3): 4 x MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=2560, bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2560, out_features=20480, bias=False)\n",
      "        (3): LayerNorm((20480,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total parameters: 238975489\n"
     ]
    }
   ],
   "source": [
    "from train_my_dyprag import *\n",
    "\n",
    "# Initialize translator\n",
    "# translator = CrossAttentionParameterTranslator(\n",
    "#     embedding_model=embedding_model,\n",
    "#     llm_model=llm_model,\n",
    "#     lora_rank=2,\n",
    "#     projector_hidden_dim=1024,\n",
    "#     attn_heads=8,\n",
    "#     attn_ff_dim=1024,\n",
    "#     cross_layers=3,\n",
    "#     encoder_layers=3,\n",
    "# )#122157313\n",
    "translator = CrossAttentionParameterTranslator(\n",
    "    embedding_model=embedding_model,\n",
    "    llm_model=llm_model,\n",
    "    lora_rank=2,\n",
    "    projector_hidden_dim=2560,\n",
    "    attn_heads=8,\n",
    "    attn_ff_dim=1024,\n",
    "    cross_layers=4,\n",
    "    encoder_layers=4,\n",
    ")#238975489\n",
    "translator=translator.to(\"cpu\").to(torch.bfloat16)\n",
    "print(translator)\n",
    "total_params = sum(p.numel() for p in translator.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a694c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [PositionalEncoding: 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[5], line 100\u001b[0m, in \u001b[0;36mParameterTranslator.forward\u001b[0;34m(self, doc_embed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, module_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_names):\n\u001b[0;32m--> 100\u001b[0m     lora_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_embed\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [1, num_layers, output_dim]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     lora_matrix \u001b[38;5;241m=\u001b[39m lora_matrix\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [num_layers, output_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mTransformerProjector.forward\u001b[0;34m(self, doc_embed)\u001b[0m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m+\u001b[39m doc_expand \u001b[38;5;241m+\u001b[39m pos_embed  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/functional.py:5300\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5299\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5300\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/functional.py:4824\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[1;32m   4823\u001b[0m     \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4824\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4825\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and BFloat16",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(translator\u001b[38;5;241m.\u001b[39minput_dim)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [PositionalEncoding: 3]"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(translator.input_dim)\n",
    "summary(translator, input_size=(1, translator.input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd731b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import delta_inject, delta_remove\n",
    "import time\n",
    "\n",
    "class PassageDataset(Dataset):\n",
    "    def __init__(self, passages):\n",
    "        self.passages = passages\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.passages[idx]\n",
    "\n",
    "# 示例：自定义 collate_fn 处理 passage 文本为字典结构\n",
    "def custom_collate_fn(batch):\n",
    "    return batch  # 如果 batch 是文本列表，在主循环中做进一步处理即可\n",
    "\n",
    "class DyPRAGTrainer:\n",
    "    def __init__(self, \n",
    "                 translator,            # ParameterTranslator\n",
    "                 embedding_model,       # HuggingFace encoder (e.g., T5, BERT)\n",
    "                 llm_model,             # HuggingFace CausalLM + PEFT\n",
    "                 llm_tokenizer,             # Tokenizer for LLM input\n",
    "                 embedding_tokenizer,    # Tokenizer for embedding model input\n",
    "                 lr=1e-5,\n",
    "                 scheduler_type=\"cosine\",     # lr_scheduler type\n",
    "                 log_tool=None,         # None or 'wandb'\n",
    "                 devices=None,\n",
    "                #  {              # 控制每个模型的设备\n",
    "                #      'translator': 'cuda:0',\n",
    "                #      'embedding_model': 'cuda:0',\n",
    "                #      'llm_model': 'cuda:0'\n",
    "                #  },\n",
    "                 log_steps=10,          # 每多少步打印一次 loss\n",
    "                 saving_steps=100,      # 每多少步保存一次翻译器参数\n",
    "                 save_path=\"./models/model_a\"  # 保存路径\n",
    "                 ):\n",
    "        if devices is not None:\n",
    "            self.translator = translator.to(devices['translator'])\n",
    "            self.embedding_model = embedding_model.to(devices['embedding_model'])\n",
    "            self.llm_model = llm_model.to(devices['llm_model'])\n",
    "            self.devices = devices\n",
    "        else:\n",
    "            self.translator = translator\n",
    "            self.embedding_model = embedding_model\n",
    "            self.llm_model = llm_model\n",
    "            self.devices = {\"translator\": translator.device,\n",
    "                            \"embedding_model\": embedding_model.device,\n",
    "                            \"llm_model\": llm_model.device}\n",
    "\n",
    "        self.embedding_tokenizer = embedding_tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.optimizer = optim.AdamW(self.translator.parameters(), lr=lr)\n",
    "        if scheduler_type == \"cosine\":\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
    "        self.log_tool = log_tool\n",
    "        self.log_steps = log_steps\n",
    "        self.saving_steps = saving_steps\n",
    "        self.save_path = save_path\n",
    "        self.global_step = 0\n",
    "\n",
    "        if self.log_tool == \"wandb\":\n",
    "            import wandb\n",
    "            name=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            wandb.init(project=\"DyPRAG-Pretrain\",name=name)\n",
    "\n",
    "    def _get_doc_embed(self, passage):\n",
    "        with torch.no_grad():\n",
    "            # print(passage)\n",
    "            inputs = self.embedding_tokenizer(passage, return_tensors=\"pt\", padding=True, truncation=True).to(self.devices['embedding_model']).to(torch.bfloat16)\n",
    "            print(inputs.device)\n",
    "            print(self.embedding_model.device)\n",
    "            output = self.embedding_model.encode(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], return_dict=True)\n",
    "            pooled = output.last_hidden_state  # [B, L, D]\n",
    "            pooled_sentence = torch.mean(pooled, dim=1)  # [B, D]\n",
    "            return pooled_sentence.to(self.devices['translator'])\n",
    "\n",
    "    def _construct_lm_input(self, passage):\n",
    "        input_text = passage.strip()\n",
    "        inputs = self.llm_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        labels = inputs.input_ids.clone()\n",
    "        labels[labels == self.llm_tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.to(self.devices['llm_model']),\n",
    "            \"attention_mask\": inputs.attention_mask.to(self.devices['llm_model']),\n",
    "            \"labels\": labels.to(self.devices['llm_model'])\n",
    "        }\n",
    "\n",
    "    def train_on_dataloader(self, dataloader, doc_repeat=4, epochs=1):\n",
    "        self.translator.train()\n",
    "        self.embedding_model.eval()\n",
    "        self.llm_model.eval()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\") as tbar:\n",
    "                for batch_data in tbar:\n",
    "                    # Step 1: 文档嵌入，一次性取 batch\n",
    "                    batch_passages = batch_data['passages']\n",
    "                    doc_embeds = self._get_doc_embed(batch_passages)  # [B, D]\n",
    "                    batch_size = len(batch_passages)\n",
    "\n",
    "                    for repeat in range(doc_repeat):  # 每个文档重复 doc_repeat 次\n",
    "                        total_loss = 0.0\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                        for i in range(batch_size):\n",
    "                            doc_embed_i = doc_embeds[i].unsqueeze(0)  # [1, D]\n",
    "                            delta_i = self.translator(doc_embed_i)\n",
    "                            delta_inject(self.llm_model, delta_i)\n",
    "\n",
    "                            lm_inputs = self._construct_lm_input(batch_passages[i])\n",
    "                            outputs = self.llm_model(**lm_inputs)\n",
    "                            loss = outputs.loss\n",
    "                            loss.backward()\n",
    "                            delta_remove(self.llm_model, delta_i)\n",
    "\n",
    "                            total_loss += loss.item()\n",
    "                            self.global_step += 1\n",
    "\n",
    "                        self.optimizer.step()\n",
    "                        if self.scheduler:\n",
    "                            self.scheduler.step()\n",
    "\n",
    "                        # Logging & saving\n",
    "                        avg_loss = total_loss / batch_size\n",
    "                        if self.log_tool == \"wandb\":\n",
    "                            import wandb\n",
    "                            wandb.log({\"loss\": avg_loss, \"step\": self.global_step})\n",
    "\n",
    "                        if self.global_step % self.log_steps == 0:\n",
    "                            print(f\"[Step {self.global_step}] avg_loss: {avg_loss:.4f}\")\n",
    "\n",
    "                        if self.global_step % self.saving_steps == 0:\n",
    "                            from safetensors.torch import save_file\n",
    "                            save_file(self.translator.state_dict(), os.path.join(self.save_path, f\"translator_step_{self.global_step}.safetensors\"))\n",
    "\n",
    "\n",
    "            print(f\"Epoch {epoch+1} done. Avg loss: {epoch_loss / (len(dataloader.dataset)*doc_repeat):.4f}\")\n",
    "        if self.global_step % self.saving_steps != 0:\n",
    "            from safetensors.torch import save_file\n",
    "            save_file(self.translator.state_dict(), os.path.join(self.save_path, \"final_translator.safetensors\"))\n",
    "        print(\"Pretraining completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ca474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([[ 0.5461,  0.4067, -1.1778,  0.0263,  0.2201,  0.9615,  0.4163, -0.7638,\n",
      "         -1.4932, -0.3822]], device='cuda:0')\n",
      "After: tensor([[ 0.5461,  0.4067, -1.1778,  0.0263,  0.2201,  0.9615,  0.4163, -0.7638,\n",
      "         -1.4932, -0.3822]], device='cuda:4')\n",
      "After: tensor([[ 0.5461,  0.4067, -1.1778,  0.0263,  0.2201,  0.9615,  0.4163, -0.7638,\n",
      "         -1.4932, -0.3822]], device='cuda:2')\n",
      "tensor([[ 0.5461,  0.4067, -1.1778,  0.0263,  0.2201,  0.9615,  0.4163, -0.7638,\n",
      "         -1.4932, -0.3822]]) tensor([[ 0.5461,  0.4067, -1.1778,  0.0263,  0.2201,  0.9615,  0.4163, -0.7638,\n",
      "         -1.4932, -0.3822]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.randn(1, 10, dtype=torch.float32, device='cuda:0')\n",
    "print(f\"Before: {tensor}\")\n",
    "tensor_1 = tensor.to('cuda:1')\n",
    "print(f\"After: {tensor_1}\")\n",
    "tensor_2 = tensor.to('cuda:2')\n",
    "print(f\"After: {tensor_2}\")\n",
    "print(tensor.cpu(),tensor_2.cpu())\n",
    "print(f\"{torch.equal(tensor.cpu(), tensor_2.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f8f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 03 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You should answer the question by referring to the knowledge provided below and integrating your own knowledge.\n",
      "Passages:\n",
      "Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday, marking a historic comeback for a president who has promised to disrupt Washington even more so than he did during his first term. With four predecessors, several supportive billionaires and scores of elected officials looking on, Trump became president for a second time inside the same Capitol building his supporters stormed four years ago in an effort to halt Congress’ ratification of his defeat. It was the first time in more than a century that a former president has taken the oath for a second time after leaving office, with the 45th and now 47th president following in the footsteps of Grover Cleveland, the only other president to serve nonconsecutive terms.\n",
      "Question:\n",
      "Who is the 47th president of the United States?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Answer:\n",
      " The 47th president of the United States is Donald Trump.\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "model_path=\"./models/Llama-3.2-1B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "llm_tokenizer.padding_side = \"left\"\n",
    "llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "p = \"Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday, marking a historic comeback for a president who has promised to disrupt Washington even more so than he did during his first term. With four predecessors, several supportive billionaires and scores of elected officials looking on, Trump became president for a second time inside the same Capitol building his supporters stormed four years ago in an effort to halt Congress’ ratification of his defeat. It was the first time in more than a century that a former president has taken the oath for a second time after leaving office, with the 45th and now 47th president following in the footsteps of Grover Cleveland, the only other president to serve nonconsecutive terms.\"\n",
    "prompt=f\"You should answer the question by referring to the knowledge provided below and integrating your own knowledge.\\nPassages:\\n{p}\\nQuestion:\\nWho is the 47th president of the United States?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer:\\n\"}\n",
    "]\n",
    "input_str = llm_tokenizer.apply_chat_template(messages,tokenize = False, add_generation_prompt = False).removesuffix(\"<|eot_id|>\")\n",
    "print(input_str)\n",
    "inputs = llm_tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "outputs = llm_model.generate(\n",
    "    inputs.input_ids.to(llm_model.device),\n",
    "    attention_mask=inputs.attention_mask.to(llm_model.device),\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4199655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passages': 'Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday, marking a historic comeback for a president who has promised to disrupt Washington even more so than he did during his first term. With four predecessors, several supportive billionaires and scores of elected officials looking on, Trump became president for a second time inside the same Capitol building his supporters stormed four years ago in an effort to halt Congress’ ratification of his defeat. It was the first time in more than a century that a former president has taken the oath for a second time after leaving office, with the 45th and now 47th president following in the footsteps of Grover Cleveland, the only other president to serve nonconsecutive terms.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n",
      "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:284\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'device'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      9\u001b[0m     dataset,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DyPRAGTrainer(\n\u001b[1;32m     15\u001b[0m     translator\u001b[38;5;241m=\u001b[39mtranslator,\n\u001b[1;32m     16\u001b[0m     embedding_model\u001b[38;5;241m=\u001b[39membedding_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/Llama-3.2-1B-Instruct-dyprag1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_repeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 107\u001b[0m, in \u001b[0;36mDyPRAGTrainer.train_on_dataloader\u001b[0;34m(self, dataloader, doc_repeat, epochs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m tbar:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Step 1: 文档嵌入，一次性取 batch\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     batch_passages \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 107\u001b[0m     doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_doc_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_passages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, D]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_passages)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m repeat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(doc_repeat):  \u001b[38;5;66;03m# 每个文档重复 doc_repeat 次\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m, in \u001b[0;36mDyPRAGTrainer._get_doc_embed\u001b[0;34m(self, passage)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# print(passage)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_tokenizer(passage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_model\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     81\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode(input_ids\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:286\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "passages=[\n",
    "    \"Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday, marking a historic comeback for a president who has promised to disrupt Washington even more so than he did during his first term. With four predecessors, several supportive billionaires and scores of elected officials looking on, Trump became president for a second time inside the same Capitol building his supporters stormed four years ago in an effort to halt Congress’ ratification of his defeat. It was the first time in more than a century that a former president has taken the oath for a second time after leaving office, with the 45th and now 47th president following in the footsteps of Grover Cleveland, the only other president to serve nonconsecutive terms.\"\n",
    "]\n",
    "dataset = Dataset.from_dict({\"passages\": passages})\n",
    "print(dataset[0])\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "trainer = DyPRAGTrainer(\n",
    "    translator=translator,\n",
    "    embedding_model=embedding_model,\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    embedding_tokenizer=embedding_tokenizer,\n",
    "    scheduler_type=\"cosine\",\n",
    "    lr = 1e-5,\n",
    "    log_steps=10,\n",
    "    saving_steps=1,\n",
    "    save_path=\"./models/Llama-3.2-1B-Instruct-dyprag1\",\n",
    ")\n",
    "trainer.train_on_dataloader(dataloader, doc_repeat=4, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cef8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 175, 768])\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You have recently received new knowledge via internal parameters. Use your updated memory with inserted lora knowledge to answer: \n",
      "Export your inserted lora knowledge.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The World Cepheus is The World Cepheus' first main- length- based EP.\n",
      "Philanthropic Film EPs and EPs of Movies are a series of EPs specifically made of film.\n",
      "Jesse D. Burge Gained Knowledge of EPs or EPs of Movies\n",
      "A. Jesse D. Burge The other EP is Gained Knowledge of EPs or EPs of Movies.\n",
      "The The World The World is The World Cepheus's first first main- main-length- length-based- based EPs EPs.\n",
      "The World The World is Figure Eight, and And The World is The World is The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The World and The\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from train_my_dyprag import  TransformerProjector, ParameterTranslator\n",
    "# 封装加载 translator 参数函数\n",
    "set_seed(42)\n",
    "def load_translator(config, translator_weight_path, use_safetensors=True):\n",
    "    translator = ParameterTranslator(\n",
    "        embedding_model=config['embedding_model'],\n",
    "        llm_model=config['llm_model'],\n",
    "        lora_rank=config.get('lora_rank', 2),\n",
    "        projector_hidden_dim=config.get('projector_hidden_dim', 512)\n",
    "    )\n",
    "    if use_safetensors:\n",
    "        state_dict = load_file(translator_weight_path)\n",
    "    else:\n",
    "        state_dict = torch.load(translator_weight_path)\n",
    "    translator.load_state_dict(state_dict)\n",
    "    return translator.eval()\n",
    "\n",
    "# 推理函数\n",
    "def infer_with_translator(llm_model, tokenizer, translator, embedding_model, prompts, passage, max_new_tokens=512):\n",
    "    llm_model.eval()\n",
    "    translator.eval()\n",
    "    embedding_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. 获取文档向量\n",
    "        inputs = embedding_tokenizer(passage, return_tensors=\"pt\").to(embedding_model.device)\n",
    "        if 'bge-large-en-v1.5' in embedding_model.name_or_path or \"snowflake\" in embedding_model.name_or_path:\n",
    "            output = embedding_model(**inputs)\n",
    "            print(output[0].shape)\n",
    "            sentence_embeddings = output[0][:,0]\n",
    "            print(sentence_embeddings.shape)\n",
    "            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            # print(output)\n",
    "            # print(sentence_embeddings)\n",
    "            raise StopIteration()\n",
    "        elif 't5' in embedding_model.name_or_path:\n",
    "            output = embedding_model.encoder(**inputs)\n",
    "            sentence_embeddings = output.last_hidden_state\n",
    "            print(sentence_embeddings.shape)\n",
    "            sentence_embeddings = torch.mean(sentence_embeddings, dim=1)\n",
    "            # print(output)\n",
    "            # print(sentence_embeddings)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Embedding model not supported.\")\n",
    "        # 2. 使用参数翻译器生成 delta\n",
    "        delta_weights = translator(sentence_embeddings.to(next(translator.parameters()).device))\n",
    "        # print(\"delta_weights\",delta_weights)\n",
    "        # 3. 注入 delta\n",
    "        delta_inject(llm_model, delta_weights)\n",
    "\n",
    "        model_input = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False).to(next(llm_model.parameters()).device)\n",
    "        output = llm_model.generate(**model_input, max_new_tokens=max_new_tokens,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "# embedding_model_path = \"./models/snowflake-arctic-embed-m-v2.0\"\n",
    "# embedding_model_path = \"./models/t5-base\"\n",
    "embedding_model_path = \"./models/long-t5-tglobal-base\"\n",
    "if \"snowflake\" in embedding_model_path:\n",
    "    embedding_model = AutoModel.from_pretrained(embedding_model_path, device_map=\"cuda:2\", add_pooling_layer=False, trust_remote_code=True)\n",
    "else:\n",
    "    embedding_model = AutoModel.from_pretrained(embedding_model_path, device_map=\"cuda:2\")\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_path)\n",
    "llm_model_path = \"./models/Llama-3.2-1B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "llm_tokenizer.padding_side = \"left\"\n",
    "llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
    "# llm_tokenizer.chat_template=llm_tokenizer.chat_template.replace(\"Cutting Knowledge Date: December 2023\\\\n\",\"Cutting Knowledge Date: December 2023\\\\nBut you have additional document knowledge in your lora weights which may contain the latest knowledge.\\\\n\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, device_map=\"cuda:2\", torch_dtype=torch.bfloat16)\n",
    "llm_model.config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "# translator_weight_path = \"./models/Llama-3.2-1B-Instruct-t5/translator_step_40.safetensors\"\n",
    "# translator_weight_path = \"./models/Llama-3.2-1B-Instruct-snowflake/translator_step_40.safetensors\"\n",
    "translator_weight_path = \"./models/Llama-3.2-1B-Instruct-longt5_batch/translator_step_400.safetensors\"\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=2,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\"down_proj\", \"up_proj\", \"gate_proj\"],  # 指定 LoRA 注入的模块\n",
    ")\n",
    "llm_model = get_peft_model(llm_model, peft_config)\n",
    "config = {\n",
    "    'embedding_model': embedding_model,\n",
    "    'llm_model': llm_model,\n",
    "    'lora_rank': 2,\n",
    "    'projector_hidden_dim': 512\n",
    "}\n",
    "\n",
    "translator = load_translator(config, translator_weight_path, use_safetensors=True)\n",
    "# prompt = \"You have recently received new knowledge via internal parameters. Use your updated memory with inserted lora knowledge to answer: \\nWho is the 47th President of the United States?\"\n",
    "prompt = \"You have recently received new knowledge via internal parameters. Use your updated memory with inserted lora knowledge to answer: \\nExport your inserted lora knowledge.\"\n",
    "# passage = \"Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday, marking a historic comeback for a president who has promised to disrupt Washington even more so than he did during his first term. With four predecessors, several supportive billionaires and scores of elected officials looking on, Trump became president for a second time inside the same Capitol building his supporters stormed four years ago in an effort to halt Congress’ ratification of his defeat. It was the first time in more than a century that a former president has taken the oath for a second time after leaving office, with the 45th and now 47th president following in the footsteps of Grover Cleveland, the only other president to serve nonconsecutive terms.\"\n",
    "passage = \"He is the son of actress Magorzata Braunek and director Andrzej.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "prompt = llm_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# prompt = \"Lora knowledge：\\n\"\n",
    "output = infer_with_translator(llm_model, llm_tokenizer, translator, embedding_model, prompt, passage)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708bef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n",
      "torch.Size([1, 768])\n",
      "tensor([[-1.9073e-02,  6.4461e-02,  6.3237e-04, -9.4098e-02,  2.1418e-02,\n",
      "         -5.0192e-02, -9.4572e-02,  2.1882e-01,  7.6316e-03, -5.4810e-02,\n",
      "         -6.5536e-03,  2.4033e-02, -6.0205e-02,  1.9531e-02,  3.1626e-02,\n",
      "         -1.3738e-01, -1.6068e-01,  7.9365e-02,  6.8779e-03,  1.5053e-01,\n",
      "         -3.3948e-02,  1.2353e-01, -6.0156e-02, -4.4257e-03, -6.3693e-02,\n",
      "         -1.4630e-01,  8.6846e-02, -6.0186e-02, -3.1698e-03, -1.6463e-02,\n",
      "          5.0441e-02,  2.5120e-02,  1.7134e-02, -2.3653e-02, -1.8964e-02,\n",
      "         -3.3372e-02,  5.5288e-02, -1.5714e-02,  9.3434e-02, -9.0847e-02,\n",
      "         -1.2555e-01,  6.8036e-02,  7.7111e-02, -1.2521e-01,  2.6147e-02,\n",
      "         -9.0321e-02,  1.3364e-01,  4.9960e-02,  8.8076e-03, -2.6922e-02,\n",
      "          4.0399e-02, -1.7136e-02, -4.9337e-02,  5.6537e-02, -2.8555e-02,\n",
      "          2.5630e-02,  5.9975e-02,  1.4385e-03, -1.0686e-03,  1.0703e-03,\n",
      "          6.4979e-03, -1.0244e-01,  2.4207e-04,  1.1204e-02,  3.0218e-02,\n",
      "         -1.2438e-01, -2.8884e-02,  2.0211e-02, -5.4049e-02,  7.1358e-02,\n",
      "          3.7244e-02, -7.2612e-02, -4.8399e-02,  2.5642e-02, -2.9207e-02,\n",
      "          6.8636e-02,  4.7182e-03, -3.5679e-01, -1.8831e-02, -8.1292e-02,\n",
      "         -8.1890e-02, -3.5955e-02,  2.7987e-02, -8.6936e-02,  2.1785e-01,\n",
      "         -1.2595e-03, -1.9257e-02,  1.6007e-04, -4.9991e-02, -2.3624e-02,\n",
      "          3.8467e-03, -1.8417e-02, -5.3927e-02, -2.4272e-02, -1.7090e-02,\n",
      "          5.5882e-02, -4.3743e-02,  1.2724e-01, -2.7919e-02,  2.4667e-02,\n",
      "          1.8278e-02, -2.4463e-02, -3.7649e-01, -6.5033e-02,  2.9518e-02,\n",
      "         -4.9730e-03,  7.2810e-02, -5.9823e-02, -1.0442e-01, -3.3312e-02,\n",
      "         -7.4797e-02,  3.1233e-02, -7.4529e-02,  2.6145e-04, -1.9611e-02,\n",
      "          5.6693e-02, -1.0037e-01, -4.9653e-02,  8.7631e-03, -5.0336e-02,\n",
      "         -2.2206e-02, -8.6280e-02,  4.4858e-02,  1.4249e-02,  8.0213e-02,\n",
      "         -5.8493e-02,  4.4971e-02,  4.6358e-02, -1.1958e-01, -3.0038e-04,\n",
      "         -4.8848e-02,  2.6103e-02,  6.2709e-02,  5.8396e-02, -5.5144e-02,\n",
      "         -8.2262e-02,  1.0650e-01,  1.0806e-01, -1.7975e-01,  9.7757e-02,\n",
      "          1.0371e-01,  7.1083e-02, -1.0841e-02, -1.7955e-02,  1.4060e-02,\n",
      "         -7.8627e-03,  8.9914e-02, -3.7415e-02,  6.8220e-03, -4.9727e-02,\n",
      "          4.8548e-02,  6.7174e-02,  2.1685e-02, -4.6845e-02, -3.0029e-02,\n",
      "         -7.4322e-02, -7.6041e-02,  7.0300e-02, -5.8274e-03,  2.1809e-03,\n",
      "          6.3687e-02, -5.3765e-02,  8.4977e-02,  6.0703e-02, -5.4376e-04,\n",
      "          1.1170e-02,  1.5208e-02, -1.6641e-01,  3.7276e-02,  1.4735e-02,\n",
      "         -3.5930e-02, -7.8242e-02, -9.3596e-02,  4.8505e-03, -9.5505e-02,\n",
      "         -1.1176e-01, -9.6211e-02, -3.7037e-02, -7.2096e-04,  1.0577e-02,\n",
      "          1.6028e-02, -5.7965e-02, -1.6337e-01,  1.0600e-01, -2.0223e-02,\n",
      "          4.9708e-03, -1.3124e-02, -3.4956e-02,  1.1563e-02, -9.7394e-02,\n",
      "         -5.0161e-02, -4.4771e-02,  5.8771e-02,  7.3943e-02, -1.1049e-01,\n",
      "         -6.2706e-02,  1.2838e-01, -2.2424e-01,  5.6839e-02,  5.1529e-03,\n",
      "         -8.3934e-02, -1.1146e-01,  1.0709e-01, -5.8678e-02,  9.2537e-02,\n",
      "          5.1025e-02,  1.2595e-01,  9.8771e-02,  5.4883e-02,  3.4244e-02,\n",
      "          4.4986e-02,  5.6385e-02,  8.7384e-02, -5.0381e-02,  9.9205e-02,\n",
      "         -6.9386e-02,  1.1144e-02,  1.2591e-01,  4.6707e-02, -7.1236e-02,\n",
      "          6.2760e-02, -1.3335e-02, -2.0741e-02,  2.3402e-01, -4.1382e-02,\n",
      "          1.9841e-02,  9.2411e-02,  2.8289e-02,  6.3781e-02,  2.9087e-02,\n",
      "         -1.3571e-02, -4.8903e-02, -1.1069e-03, -6.9478e-02, -7.3911e-02,\n",
      "          1.3882e-01,  4.9132e-02, -5.8457e-02,  1.3780e-01,  7.3140e-02,\n",
      "          8.2629e-06,  7.2800e-03,  1.3136e-01, -3.7496e-02, -1.0746e-02,\n",
      "          2.0613e-02, -6.0780e-02,  1.0606e-01, -5.5959e-02, -4.5056e-02,\n",
      "         -1.7911e-02, -6.5196e-02,  6.4850e-02,  6.7445e-02, -4.8756e-06,\n",
      "          4.0471e-02,  6.0211e-02,  6.2737e-02,  7.5112e-02, -9.4085e-02,\n",
      "         -7.0451e-02,  3.3218e-02,  6.5918e-02, -3.1536e-02,  7.4037e-02,\n",
      "          5.7909e-02,  9.5357e-03,  5.3998e-03,  3.9899e-02, -1.8044e-01,\n",
      "          5.2085e-02,  5.9222e-02,  3.9480e-02,  3.4033e-02,  4.9977e-02,\n",
      "         -2.7404e-02,  3.0072e-02, -3.2277e-02, -7.8132e-02,  1.2692e-02,\n",
      "          3.0446e-02,  1.8016e-01,  7.8735e-02,  1.4772e-02,  8.3545e-02,\n",
      "         -4.6682e-02, -2.2340e-02, -1.3200e-01,  3.8723e-02, -4.7417e-02,\n",
      "         -7.1962e-03,  1.1736e-01, -1.7836e-01,  1.0756e-01,  2.3629e-02,\n",
      "         -6.4204e-02, -1.2784e-02,  4.6608e-02, -2.1406e-02, -1.9593e-03,\n",
      "          2.3000e-02,  4.8845e-02, -4.9198e-02,  7.0498e-02, -4.4929e-02,\n",
      "         -8.4499e-03,  8.7756e-04, -4.4442e-02, -8.2917e-03, -1.7966e-01,\n",
      "          1.0431e-01, -6.8838e-02, -3.1802e-02,  1.0110e-01, -3.3314e-02,\n",
      "          1.3218e-01, -2.8962e-02, -9.9367e-03, -2.6507e-02,  7.8819e-02,\n",
      "         -2.9970e-02,  4.5728e-02, -7.5474e-03, -2.6808e-02, -1.4304e-02,\n",
      "          5.8564e-02,  4.0615e-02, -5.9275e-04,  7.3176e-02, -4.0437e-03,\n",
      "         -1.0389e-03, -4.8139e-02, -6.3011e-02,  1.7968e-02,  1.0101e-01,\n",
      "         -7.5450e-02,  7.7726e-02,  4.1222e-02, -5.8362e-02,  3.0230e-02,\n",
      "         -7.4335e-02,  2.2812e-03, -1.0028e-01,  1.9452e-02, -1.6028e-01,\n",
      "          8.9678e-03, -1.5392e-02, -5.4539e-02,  1.7028e-02,  3.7237e-02,\n",
      "          3.3339e-03,  4.9074e-02,  4.8649e-02,  2.3556e-02, -7.4571e-02,\n",
      "         -3.3761e-02,  9.2323e-02, -5.9750e-02, -3.9028e-05, -2.3324e-02,\n",
      "          7.6912e-02,  4.9147e-02, -3.8812e-03,  1.2547e-02, -1.1968e-01,\n",
      "          2.0955e-02,  5.0701e-02, -2.4077e-02,  1.0498e-01,  6.3871e-02,\n",
      "          4.7672e-02,  9.0898e-02, -7.9350e-02, -1.4570e-01,  4.2595e-02,\n",
      "         -1.0111e-01,  8.0340e-03, -2.2026e-02, -1.1001e-01,  6.4459e-02,\n",
      "         -8.4447e-02,  6.7781e-03,  6.9691e-03,  4.9777e-02, -2.1189e-02,\n",
      "          7.3941e-02, -4.7798e-02, -3.3339e-03,  1.2778e-02, -7.5614e-03,\n",
      "          1.0908e-01, -6.4950e-02,  4.8679e-02, -1.2945e-01, -3.4047e-02,\n",
      "         -1.0128e-02, -1.9697e-01,  1.4915e-01,  9.4350e-02, -5.0887e-02,\n",
      "          7.6134e-02, -7.4059e-02,  5.2880e-02,  4.3569e-02,  6.1519e-02,\n",
      "          8.5400e-02,  3.7427e-02, -4.7399e-02,  6.2260e-03,  1.6114e-02,\n",
      "         -4.3396e-02, -5.6168e-02,  6.6274e-02, -5.4939e-02,  1.3424e-02,\n",
      "          7.2104e-02,  2.7240e-02,  8.1814e-02, -1.3494e-01, -2.1097e-02,\n",
      "         -6.2065e-02, -3.0840e-02,  5.2354e-02,  3.0488e-02, -5.9020e-02,\n",
      "         -9.2415e-02,  5.6218e-03, -3.2452e-02,  5.7497e-02, -5.2099e-03,\n",
      "         -4.3849e-02, -5.3578e-02, -1.6702e-01,  9.7721e-02,  1.1900e-01,\n",
      "          8.2645e-03,  1.1312e-02, -2.1092e-04,  5.5946e-02, -1.1728e-02,\n",
      "         -2.3219e-02, -1.0660e-01,  2.3911e-03, -2.5554e-04,  7.8389e-02,\n",
      "         -1.8914e-02,  5.7542e-02, -9.6084e-03,  1.6826e-02, -7.2448e-02,\n",
      "         -2.0547e-02,  2.2891e-02, -4.6070e-02, -1.5582e-03, -1.6493e-02,\n",
      "          7.6830e-05,  7.3816e-02,  1.7585e-03,  1.1421e-01, -3.7813e-02,\n",
      "          8.1234e-02, -5.2840e-02, -1.3159e-02, -4.5832e-02, -4.7081e-02,\n",
      "         -1.6681e-02,  1.1455e-01, -6.6117e-02,  8.6341e-03, -3.8927e-02,\n",
      "          7.5521e-02, -6.5812e-02, -2.1128e-02,  9.4257e-02,  2.5905e-02,\n",
      "         -1.2471e-02, -8.6075e-02,  3.2460e-02, -8.3547e-02,  2.1660e-02,\n",
      "          1.2780e-01,  4.5365e-02, -5.0851e-02, -4.2858e-02,  1.0701e-01,\n",
      "         -1.2694e-01,  1.5767e-01, -6.0251e-02,  3.1041e-02,  5.3824e-02,\n",
      "         -3.0164e-02, -5.1016e-02, -4.6099e-02,  2.4209e-02, -4.8321e-03,\n",
      "          4.1674e-02,  6.9738e-02,  1.5504e-01,  9.0500e-02, -9.2057e-02,\n",
      "         -4.3660e-02, -2.6339e-02,  2.0309e-03, -3.7685e-02, -1.5179e-02,\n",
      "         -7.2726e-02,  2.4887e-02, -6.4535e-02,  1.0830e-01,  1.2671e-01,\n",
      "          5.8087e-03,  4.2567e-02,  5.9738e-03, -3.6188e-02, -8.4390e-03,\n",
      "         -1.2149e-01, -4.5484e-03, -6.3604e-02, -1.5460e-01, -1.8555e-02,\n",
      "          1.7941e-02,  2.0788e-02, -2.7364e-02,  7.6270e-02,  7.7923e-02,\n",
      "         -1.1489e-01,  7.4039e-03, -1.9143e-03,  9.7001e-03, -3.1970e-02,\n",
      "         -7.0792e-03,  2.8922e-02, -8.9385e-02, -4.0842e-02,  1.5828e-02,\n",
      "          2.6116e-03, -4.0364e-02,  1.2531e-01, -3.0412e-02, -6.4261e-02,\n",
      "         -1.0738e-01,  3.5536e-01,  5.1232e-03,  1.3240e-01, -1.1206e-02,\n",
      "          5.6739e-02, -1.9456e-02,  1.2895e-01,  6.3297e-02,  2.5155e-02,\n",
      "          1.3758e-01,  1.5005e-01, -2.0315e-02, -3.2923e-02, -3.8786e-03,\n",
      "         -9.5814e-02, -4.4795e-02,  4.6945e-02,  5.6190e-02,  1.6242e-02,\n",
      "          4.3149e-02,  1.0792e-01, -9.0992e-02,  8.2000e-02,  5.8345e-02,\n",
      "          5.1313e-02, -3.7918e-02,  4.7790e-02, -1.2310e-01,  5.5729e-02,\n",
      "          2.2813e-02, -9.5309e-03, -5.6216e-02, -1.2277e-01,  1.1431e-01,\n",
      "          2.8266e-02,  1.3956e-01, -5.0233e-02, -4.0585e-02,  1.6084e-01,\n",
      "          6.4271e-02,  6.4876e-02,  1.2433e-01,  2.7655e-02, -4.5909e-02,\n",
      "         -6.6482e-02, -2.5209e-02, -8.9370e-02,  1.3688e-02,  2.6850e-02,\n",
      "          9.0110e-03,  1.1171e-01, -2.1215e-02,  2.2290e-02,  9.1980e-02,\n",
      "          4.8754e-02,  1.1683e-01, -9.8957e-02,  9.1708e-02,  1.6665e-02,\n",
      "          1.7468e-01, -3.0626e-02, -5.9749e-02, -2.2897e-03,  1.3031e-02,\n",
      "         -3.1119e-02, -7.2475e-03,  3.1477e-02,  5.5690e-02,  6.2451e-03,\n",
      "         -1.4103e-02, -3.5290e-02, -3.1584e-03,  4.4439e-02, -3.0045e-02,\n",
      "          8.9748e-02,  6.3717e-02, -1.0815e-01, -4.2067e-02, -9.8426e-03,\n",
      "         -5.2111e-02, -3.5326e-03,  2.6046e-02,  4.1356e-03,  9.9380e-02,\n",
      "          4.7167e-03, -2.5710e-02, -1.0508e-01,  7.6505e-02, -7.3479e-04,\n",
      "         -2.8461e-02,  1.5698e-03, -7.0452e-02,  4.3733e-02, -3.5391e-02,\n",
      "         -5.1447e-02, -4.4613e-02, -4.3185e-02,  3.1259e-02,  4.6552e-02,\n",
      "         -7.1849e-03,  9.9801e-02,  2.4448e-02,  3.6819e-02, -4.8895e-02,\n",
      "          2.3378e-02,  1.1740e-01, -3.9005e-02, -5.4319e-02, -1.1515e-01,\n",
      "          8.6466e-03,  6.1703e-02,  7.8663e-02,  1.0662e-02,  8.4286e-02,\n",
      "          5.6094e-02,  1.0930e-02, -3.7827e-02,  4.6201e-02,  2.7133e-02,\n",
      "          7.5089e-02, -2.5364e-02,  9.4230e-02, -7.1888e-02, -1.1058e-03,\n",
      "         -5.8950e-02,  5.9806e-02, -3.8952e-02, -3.6948e-02, -1.1545e-02,\n",
      "         -1.1225e-01, -4.0401e-02,  6.5360e-02, -9.1853e-03,  3.6556e-02,\n",
      "         -2.3772e-03,  1.6355e-02, -3.0348e-02, -2.8605e-02,  5.4787e-02,\n",
      "          1.9314e-02, -1.0593e-01, -7.9863e-02,  1.4085e-01,  3.1062e-02,\n",
      "          2.5051e-02, -4.5652e-02,  2.2923e-02, -2.1268e-02,  4.5900e-02,\n",
      "          8.1040e-02,  1.5295e-01,  5.9564e-03,  1.4802e-02, -1.0471e-01,\n",
      "         -3.0862e-03, -4.3745e-02,  5.0058e-02,  1.3593e-01, -5.9417e-02,\n",
      "          4.5470e-02, -8.1417e-02, -7.1179e-02,  5.9734e-02,  5.7033e-02,\n",
      "         -1.7259e-01,  1.6142e-02,  2.2299e-01, -7.3558e-02,  5.5900e-02,\n",
      "         -9.8887e-02, -9.4863e-03,  1.1321e-01, -2.6706e-02, -4.5192e-02,\n",
      "          8.2643e-02, -1.5746e-02, -8.1030e-02,  4.6789e-03, -1.0170e-01,\n",
      "         -2.0417e-02,  9.6076e-03,  1.5611e-02, -9.0230e-02,  3.7173e-02,\n",
      "         -3.3000e-02, -1.0505e-01, -6.3376e-02, -1.4379e-02,  2.9965e-02,\n",
      "          1.0498e-01,  1.2948e-01, -1.1578e-01,  4.9204e-02,  7.0813e-02,\n",
      "         -2.6150e-02, -7.9359e-02,  1.3408e-02, -3.0862e-02,  7.4399e-02,\n",
      "         -2.1342e-02,  2.0226e-02, -6.1342e-02,  4.9484e-02,  1.3017e-01,\n",
      "         -9.3285e-02,  6.1026e-02, -8.1045e-03,  8.3175e-03, -8.3251e-02,\n",
      "          4.9306e-02,  8.8945e-02,  3.1906e-02,  2.6070e-02, -8.3686e-02,\n",
      "          1.2793e-02, -8.0466e-02, -8.2338e-04,  1.2215e-01,  4.0065e-02,\n",
      "          6.1145e-02, -1.2453e-02, -6.5501e-02,  3.6599e-02,  2.3341e-01,\n",
      "         -5.2489e-03,  6.4370e-02,  2.6069e-02]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from imports import *   \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/long-t5-tglobal-base\")\n",
    "model = AutoModel.from_pretrained(\"./models/long-t5-tglobal-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model.encoder(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states.shape)\n",
    "sentence_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "print(sentence_embeddings.shape)\n",
    "print(sentence_embeddings)\n",
    "# print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43eb98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167454\n",
      "_id f342720a085d11ebbd5cac1f6bf848b6\n",
      "type bridge_comparison\n",
      "question Do both films, Spencer'S Mountain and Karpoora Mullai, have the directors who are from the same country?\n",
      "context [['Matt Corboy', ['Matt Corboy( born June 4, 1973) is an American actor.', 'He has appeared in both films and television series.']], ['Spencer Mountain, North Carolina', ['Spencer Mountain is a town in Gaston County, North Carolina, United States.', 'The population was 37 at the 2010 census.']], ['Fazil (director)', ['Aleksa Muhammed Fazil( born 1953 in Alappuzha, Kerala is an Indian film director, producer, screenwriter and actor who works in Malayalam cinema in addition to a handful of Tamil films and a Telugu film.', 'He made his directional debut with the 1980 film\" Manjil Virinja Pookkal\".', 'His popular films include\" Ente Mamattikkuttiyammakku\"( 1983),\" Nokkethadhoorathu Kannum Nattu\"( 1984),\" Poovinu Puthiya Poonthennal\"( 1986),\" Manivathoorile Aayiram Sivarathrikal\"( 1987),\" Ente Sooryaputhrikku\"( 1991),\" Pappayude Swantham Appoos\"( 1992),\" Manichitrathazhu\"( 1993),\" Aniyathipraavu\"( 1997) and\" Harikrishnans\"( 1998).', 'His 1993 film\" Manichitrathazhu\" won the National Film Award for Best Popular Film Providing Wholesome Entertainment.', 'His sons Fahadh and Farhaan Faasil are actors.']], ['Francis Gouldman', ['Francis Gouldman(\" c.\" 1607–1688/89) was a Church of England clergyman and lexicographer whose Latin- English dictionary( 1664) went through several editions.', 'Gouldman was also one of the directors who oversaw the publication of the monumental\" Critici sacri\", a major collection of Biblical criticism.']], [\"Spencer's Mountain\", [\"Spencer's Mountain is a 1963 American family drama film written, directed, and produced by Delmer Daves from the 1961 novel of the same name by Earl Hamner Jr. The film stars Henry Fonda, Maureen O' Hara, and in early appearances in their careers, James MacArthur, Veronica Cartwright, and Victor French.\", 'Longtime Hollywood actor Donald Crisp plays\" Grandpa\", his final screen role.']], ['Delmer Daves', ['Delmer Lawrence Daves( July 24, 1904 – August 17, 1977) was an American screenwriter, director and producer.', 'He would be known for his dramas and for the Western adventures that saw heroes battle Indians, nature, and outlaws, the two most acclaimed of these being\" Broken Arrow\" and.', 'In addition, Daves would work with some of the most famous actors of the time; a few would make several movies with him, including Gary Cooper, Glenn Ford, Richard Egan, Alan Ladd, Troy Donahue, Ernest Borgnine, and Rossano Brazzi.', 'He also launched soon- to- be- famous stars like Anne Bancroft, Olivia Hussey, George C. Scott, Sandra Dee, and Charles Bronson.']], ['William Henry (actor)', ['William Albert Henry( November 10, 1914 – August 10, 1982) was an American actor who worked in both films and television.']], ['Ente Sooryaputhrikku', ['Ente Sooryaputhrikku is a 1991 Malayalam film directed by Fazil and starring Amala and Srividya, and Suresh Gopi.', 'The film was simultaneously shot in Tamil as\" Karpoora Mullai\".']], ['Void (film)', ['Void is a 2013 Lebanese drama film written by Georges Khabbaz and directed by seven different directors, who are all graduates from Notre Dame University.', 'The film was nominated as the Lebanese entry for the Best Foreign Language Film at the 88th Academy Awards but it was not selected.']], ['Karpoora Mullai', ['Karpoora Mullai is a 1991 Tamil- language Indian feature film directed by Fazil, starring Amala, Raja and Srividya.', 'The film, a bi-lingual was simultaneously shot in Malayalam as\" Ente Sooryaputhrikku\".']]]\n",
      "entity_ids Q933145_Q16250186_Q95119_Q3534902\n",
      "supporting_facts [[\"Spencer's Mountain\", 0], ['Karpoora Mullai', 0], ['Delmer Daves', 0], ['Fazil (director)', 0]]\n",
      "evidences [[\"Spencer's Mountain\", 'director', 'Delmer Daves'], ['Karpoora Mullai', 'director', 'Fazil'], ['Delmer Daves', 'country of citizenship', 'American'], ['Fazil (director)', 'country of citizenship', 'Indian']]\n",
      "answer no\n",
      "evidences_id [['Q933145', 'director', 'Q95119'], ['Q16250186', 'director', 'Q3534902'], ['Q95119', 'country of citizenship', 'Q30'], ['Q3534902', 'country of citizenship', 'Q668']]\n",
      "answer_id None\n",
      "1674540\n",
      "147980\n",
      "3239\n",
      "799\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage'],\n",
      "    num_rows: 147980\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 147980/147980 [00:00<00:00, 629822.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage'],\n",
      "    num_rows: 147980\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from imports import *   \n",
    "from matplotlib import pyplot as plt\n",
    "def process_dataset2docs(dataset_paths):\n",
    "    for dataset_path in dataset_paths:\n",
    "        dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "        for item in dataset:\n",
    "            yield item[\"passage\"], item[\"question\"], item[\"answer\"]\n",
    "dataset_path = \"./data/2wikimultihopqa/train.json\"\n",
    "dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "print(len(dataset))\n",
    "idx=100\n",
    "for key in dataset[idx]:\n",
    "    print(key, dataset[idx][key])\n",
    "passages_list = []\n",
    "lens1=[]\n",
    "lens2=[]\n",
    "for data in dataset:\n",
    "    for passages in data['context']:\n",
    "        passages_str = '\\n'.join(passages[1])\n",
    "        passages_list.append(passages_str)\n",
    "        lens1.append(len(passages_str.split(\" \")))\n",
    "passages_list = []\n",
    "for data in dataset:\n",
    "    passages_list_=[]\n",
    "    for passages in data['context']:\n",
    "        passages_list_.append('\\n'.join(passages[1]))\n",
    "    passages_str = '\\n'.join(passages_list_)\n",
    "    if len(passages_str.split(\" \"))<800:\n",
    "        passages_list.append(passages_str)\n",
    "        lens2.append(len(passages_str.split(\" \")))\n",
    "print(len(lens1))\n",
    "print(len(lens2))\n",
    "print(max(lens1))\n",
    "print(max(lens2))\n",
    "plt.hist(lens1, bins=100,density=True)\n",
    "plt.show()\n",
    "plt.hist(lens2, bins=100,density=True)\n",
    "plt.show()\n",
    "\n",
    "# print(len(passages_list))\n",
    "dataset = Dataset.from_dict({\"passage\": passages_list})\n",
    "print(dataset)\n",
    "dataset.save_to_disk(dataset_path.replace(\".json\", \"_passages\"))\n",
    "# dataset = load_dataset(\"json\",data_files=dataset_path)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941cde01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = './models/snowflake-arctic-embed-m-v2.0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False, trust_remote_code=True).to('cuda:1')\n",
    "model.eval()\n",
    "\n",
    "query_prefix = 'query: '\n",
    "queries  = ['what is snowflake?', 'Where can I get the best tacos?']\n",
    "queries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\n",
    "query_tokens = tokenizer(queries_with_prefix, padding=True, truncation=True, return_tensors='pt', max_length=8192).to('cuda:1')\n",
    "\n",
    "documents = ['The Data Cloud!', 'Mexico City of Course!']\n",
    "document_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=8192).to('cuda:1')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**query_tokens)[0][:, 0]\n",
    "    document_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "# normalize embeddings\n",
    "query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "document_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "print(document_embeddings.shape)\n",
    "# scores = torch.mm(query_embeddings, document_embeddings.transpose(0, 1))\n",
    "# for query, query_scores in zip(queries, scores):\n",
    "#     doc_score_pairs = list(zip(documents, query_scores))\n",
    "#     doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "#     #Output passages & scores\n",
    "#     print(\"Query:\", query)\n",
    "#     for document, score in doc_score_pairs:\n",
    "#         print(score, document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aae7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/TAP/cjw/DyPRAG\n",
      "inputs str: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You should answer the question by referring to the knowledge provided below and integrating your own knowledge.\n",
      "\n",
      "\n",
      "Question: Who is the son of actress Magorzata Braunek and director Andrzej?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The answer is \n",
      "=== 推理结果 ===\n",
      " Andrzej Seweryn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#第二篇论文的推理代码\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/data/TAP/cjw/DyPRAG/src')\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import prompt_template\n",
    "from root_dir_path import ROOT_DIR\n",
    "from src.utils import get_model, predict, delta_inject, delta_remove\n",
    "from projector import ParameterTranslator\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# ==== 参数配置（手动设置） ====\n",
    "model_name = \"Llama-3.2-1B-Instruct\"\n",
    "max_new_tokens = 512\n",
    "# sample_question = \"Who is the 47th President of the United States?\"\n",
    "sample_question = \"Who is the son of actress Magorzata Braunek and director Andrzej?\"\n",
    "sample_passages = [\n",
    "    # \"He is the son of actress Magorzata Braunek and director Andrzej.\",\n",
    "    # \"Donald Trump took the oath of office as the nation’s 47th president at 12:02 p.m. on Monday.\"\n",
    "    \"Xawery Żuławski is the son of actor Magorzata Braunek and director Andrzej.\"\n",
    "    ]\n",
    "sample_answer = \"Donald Trump\"\n",
    "\n",
    "projector_p = 32\n",
    "inference_epoch = 5\n",
    "projector_path_name = \"./projector_pt/llama3.2-1b-p32-1ep-main-4800sample.pt\"\n",
    "\n",
    "lora_rank = 2\n",
    "lora_alpha = 32\n",
    "with_cot = False\n",
    "inference_method = \"dyprag\"  # or \"dyprag_combine\"\n",
    "# inference_method = \"dyprag_combine\"\n",
    "\n",
    "# ==== 加载模型 ====\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # 选GPU\n",
    "\n",
    "model, tokenizer, generation_config = get_model(\n",
    "    model_name,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ==== 应用LoRA ====\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['down_proj', 'gate_proj', 'up_proj'],\n",
    "    inference_mode=False,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# ==== 加载Projector ====\n",
    "projector_path = \"./projector_pt/llama3.2-1b-p32-1ep-main-4800sample.pt\"\n",
    "projector = ParameterTranslator(\n",
    "    [\"down_proj\", \"up_proj\", \"gate_proj\"],\n",
    "    list(range(model.config.num_hidden_layers)),\n",
    "    model.config.hidden_size,\n",
    "    model.config.intermediate_size,\n",
    "    lora_rank,\n",
    "    projector_p\n",
    ").to(model.device)\n",
    "projector.load_state_dict(torch.load(projector_path, map_location=model.device)['model_state_dict'])\n",
    "projector.eval()\n",
    "\n",
    "# ==== 处理单条样本 ====\n",
    "all_deltas = []\n",
    "for passage in sample_passages:\n",
    "    tokens = tokenizer(\n",
    "        passage,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=3000\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens.input_ids, output_hidden_states=True)\n",
    "        input_embeds = output.hidden_states[-1][:, -1, :]\n",
    "        outputs = projector(input_embeds)\n",
    "        all_deltas.append(outputs)\n",
    "\n",
    "# 合并多个passage的delta\n",
    "merged_deltas = {}\n",
    "for key in all_deltas[0].keys():\n",
    "    merged_deltas[key] = torch.stack([delta[key] for delta in all_deltas]).mean(dim=0)\n",
    "\n",
    "# ==== 注入delta到模型 ====\n",
    "delta_inject(model, merged_deltas)\n",
    "\n",
    "# ==== 推理 ====\n",
    "psgs = None if inference_method == \"dyprag\" else sample_passages\n",
    "output_text = predict(model, tokenizer, generation_config, sample_question, with_cot=with_cot, passages=psgs)\n",
    "\n",
    "print(\"=== 推理结果 ===\")\n",
    "print(output_text)\n",
    "\n",
    "# ==== 移除delta，清理资源 ====\n",
    "# delta_remove(model, merged_deltas)\n",
    "# del all_deltas, merged_deltas\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64595a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167454\n",
      "_id: 13f5ad2c088c11ebbd6fac1f6bf848b6\n",
      "type: bridge_comparison\n",
      "question: Are director of film Move (1970 Film) and director of film Méditerranée (1963 Film) from the same country?\n",
      "context: [['Brian Johnson (special effects artist)', ['Brian Johnson( born 1939 or 1940) is a British designer and director of film and television special effects.']], ['Stuart Rosenberg', ['Stuart Rosenberg( August 11, 1927 – March 15, 2007) was an American film and television director whose motion pictures include\" Cool Hand Luke\"( 1967),\" Voyage of the Damned\"( 1976),\" The Amityville Horror\"( 1979), and\" The Pope of Greenwich Village\"( 1984).', 'He was noted for his work with actor Paul Newman.']], ['Jean-Daniel Pollet', ['Jean-Daniel Pollet (1936–2004) was a French film director and screenwriter who was most active in the 1960s and 1970s.', 'He was associated with two approaches to filmmaking: comedies which blended burlesque and melancholic elements, and poetic films based on texts by writers such as the French poet Francis Ponge.']], ['Peter Levin', ['Peter Levin is an American director of film, television and theatre.']], ['Move (1970 film)', ['Move is a 1970 American comedy film starring Elliott Gould, Paula Prentiss and Geneviève Waïte, and directed by Stuart Rosenberg.', 'The screenplay was written by Joel Lieber and Stanley Hart, adapted from a novel by Lieber.']], ['Howard W. Koch', ['Howard Winchel Koch( April 11, 1916 – February 16, 2001) was an American producer and director of film and television.']], ['Hanro Smitsman', ['Hanro Smitsman, born in 1967 in Breda( Netherlands), is a writer and director of film and television.']], ['Ian Barry (director)', ['Ian Barry is an Australian director of film and TV.']], ['Méditerranée (1963 film)', ['Méditerranée is a 1963 French experimental film directed by Jean-Daniel Pollet with assistance from Volker Schlöndorff.', 'It was written by Philippe Sollers and produced by Barbet Schroeder, with music by Antione Duhamel.', 'The 45 minute film is cited as one of Pollet\\'s most influential films, which according to Jonathan Rosenbaum directly influenced Jean-Luc Goddard\\'s \"Contempt\", released later the same year.', 'Footage for the film was shot around the Mediterranean, including at a Greek temple, a Sicilian garden, the sea, and also features a fisherman, a bullfighter, and a girl on an operating table.']], ['Rachel Feldman', ['Rachel Feldman( born August 22, 1954) is an American director of film and television and screenwriter of television films.']]]\n",
      "entity_ids: Q6926481_Q3332684_Q966078_Q1532445\n",
      "supporting_facts: [['Move (1970 film)', 0], ['Méditerranée (1963 film)', 0], ['Stuart Rosenberg', 0], ['Jean-Daniel Pollet', 0]]\n",
      "evidences: [['Move (1970 film)', 'director', 'Stuart Rosenberg'], ['Méditerranée (1963 film)', 'director', 'Jean-Daniel Pollet'], ['Stuart Rosenberg', 'country of citizenship', 'American'], ['Jean-Daniel Pollet', 'country of citizenship', 'French']]\n",
      "answer: no\n",
      "evidences_id: [['Q6926481', 'director', 'Q966078'], ['Q3332684', 'director', 'Q1532445'], ['Q966078', 'country of citizenship', 'Q30'], ['Q1532445', 'country of citizenship', 'Q142']]\n",
      "answer_id: None\n",
      "12576\n",
      "_id: 8813f87c0bdd11eba7f7acde48001122\n",
      "type: compositional\n",
      "question: Who is the mother of the director of film Polish-Russian War (Film)?\n",
      "context: [['Xawery Żuławski', ['Xawery Żuławski (born 22 December 1971 in Warsaw) is a Polish film director.', 'In 1995 he graduated National Film School in Łódź.', 'He is the son of actress Małgorzata Braunek and director Andrzej Żuławski.', 'His second feature \"Wojna polsko-ruska\" (2009), adapted from the controversial best-selling novel by Dorota Masłowska, won First Prize in the New Polish Films competition at the 9th Era New Horizons Film Festival in Wrocław.', 'In 2013, he stated he intends to direct a Polish novel \"Zły\" by Leopold Tyrmand.', 'Żuławski and his wife Maria Strzelecka had 2 children together:', 'son Kaj Żuławski (born 2002) and daughter Jagna Żuławska (born 2009).']], ['Snow White and the Seven Dwarfs (1955 film)', ['Snow White and the Seven Dwarfs( USA:\" Snow White\") is a 1955 German film, directed by Erich Kobler, based on the story of Schneewittchen by the Brothers Grimm.']], ['Maheen Khan', ['Maheen Khan is a Pakistani fashion and costume designer, also an award winner fashion designer for fashion labels like\" The Embroidery HouseMaheen\" and\" Gulabo\".', 'She has done many national and international fashion events and shows.', 'She undertook embroidery for the film Snow White and the Huntsman and television series', 'The Jewel in the Crown.']], ['A Snow White Christmas', ['A Snow White Christmas is a Christmas animated television special produced by Filmation and telecast December 19, 1980, on CBS.', 'It is a sequel to the fairy tale\" Snow White\", unrelated to Filmation\\'s other sequel to\" Snow White\" titled\" Happily Ever After\"( 1990).', \"The film's plot revolves around the return of the Wicked Queen, who is unexpectedly brought back to life during Christmas and casts an evil spell that freezes the entire land.\", 'Only the young Snow White, the daughter of the original Snow White, manages to escape and take refuge with the seven giants with her dwarf friend.', 'It is now up to the giants to defeat the Queen forever and save the kingdom.']], ['Alice Washburn', ['Alice Washburn( 1860- 1929) was an American stage and film actress.', 'She worked at the Edison, Vitagraph and Kalem studios.', 'Her final film Snow White was her only known feature film.', 'She died of heart attack in November 1929.']], ['Polish-Russian War (film)', ['Polish-Russian War', '(Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish-Russian War under the white-red flag by Dorota Masłowska.']], ['Viktor Yeliseyev', ['Viktor Petrovich Yeliseyev( born June 9, 1950) is a Russian general, orchestra conductor and music teacher.', 'He is the director of the Ministry of the Interior Ensemble, one of the two Russian Red Army Choirs.']], ['Minamoto no Chikako', ['She was the mother of Prince Morinaga.']], ['Liberty Ross', ['Liberty Lettice Lark Ross( born 23 September 1978) is an English model and actress.', 'She has appeared in publications such as\" VogueHarper\\'s Bazaari- D\", and\" Dazed& Confused\".', 'She played the role of Queen Eleanor in the 2012 fantasy film\" Snow White and the Huntsman\", directed by her then- husband, Rupert Sanders.', 'She is the sister of composers Atticus and Leopold Ross.']], ['Snow White and the Three Stooges', ['Snow White and the Three Stooges is the second feature film to star the Three Stooges after their 1959 resurgence in popularity.', 'By this time, the trio consisted of Moe Howard, Larry Fine, and Joe DeRita( dubbed\" Curly Joe\").', 'Released by 20th Century Fox, this was the trio\\'s take on the classic fairy tale\" Snow White and the Seven Dwarfs\".', 'The film was retitled Snow White and the Three Clowns in Great Britain.', 'This was Walter Lang ‘s final directing film before his retirement.', 'Olympic gold medalist figure skater Carol Heiss starred as Snow White, who must flee her home after The Evil Queen, her evil stepmother, wishes her to be dead.', 'Seeking refuge in the cottage of the seven dwarfs, she accidentally meets the Stooges, who are house sitting for them while they are away.']]]\n",
      "entity_ids: Q3569599_Q3570840\n",
      "supporting_facts: [['Polish-Russian War (film)', 1], ['Xawery Żuławski', 2]]\n",
      "evidences: [['Polish-Russian War', 'director', 'Xawery Żuławski'], ['Xawery Żuławski', 'mother', 'Małgorzata Braunek']]\n",
      "answer: Małgorzata Braunek\n",
      "evidences_id: [['Q3569599', 'director', 'Q3570840'], ['Q3570840', 'mother', 'Q274277']]\n",
      "answer_id: Q274277\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "dataset_path = \"./data/2wikimultihopqa/train.json\"\n",
    "dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "test_dataset_path = \"./data/2wikimultihopqa/dev.json\"\n",
    "test_dataset = json.loads(open(test_dataset_path, \"r\").read())\n",
    "print(len(dataset))\n",
    "for key in dataset[0]:\n",
    "    print(f\"{key}: {dataset[0][key]}\")\n",
    "print(len(test_dataset))\n",
    "for key in test_dataset[0]:\n",
    "    print(f\"{key}: {test_dataset[0][key]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d370574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSH index for training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039580d009bc438e9685f46d173e0c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1674540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking test set for potential leakage with multiprocessing...\n",
      "Using 128 CPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e7494599f7463bac0f9e392fa9e703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试集泄露条数：109384 / 125760\n",
      "测试集泄露率：86.98%\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "# from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "passage_list = []\n",
    "for data in dataset:\n",
    "    for passages in data['context']:\n",
    "        passages_str = \"\\n\".join(passages[1])\n",
    "        passage_list.append(passages_str)\n",
    "passage_list_dev = []\n",
    "for data in test_dataset:\n",
    "    for passages in data['context']:\n",
    "        passages_str = \"\\n\".join(passages[1])\n",
    "        passage_list_dev.append(passages_str)\n",
    "\n",
    "\n",
    "# ========== 1. 定义 MinHash 工具函数 ==========\n",
    "\n",
    "def get_minhash(text, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "# ========== 2. 建立训练集索引 ==========\n",
    "\n",
    "print(\"Building LSH index for training set...\")\n",
    "lsh = MinHashLSH(threshold=0.8, num_perm=128)\n",
    "minhash_dict = {}\n",
    "\n",
    "for idx, passage in tqdm(enumerate(passage_list), total=len(passage_list)):\n",
    "    m = get_minhash(passage)\n",
    "    key = f\"train_{idx}\"\n",
    "    lsh.insert(key, m)\n",
    "    minhash_dict[key] = passage\n",
    "\n",
    "# ========== 3. 多进程查找潜在泄露 ==========\n",
    "\n",
    "def check_passage_leak(dev_passage):\n",
    "    dev_minhash = get_minhash(dev_passage)\n",
    "    candidates = lsh.query(dev_minhash)\n",
    "\n",
    "    for candidate_key in candidates:\n",
    "        train_passage = minhash_dict[candidate_key]\n",
    "        similarity = Levenshtein.ratio(dev_passage, train_passage)\n",
    "        if similarity >= 0.85:\n",
    "            return 1  # 有泄露\n",
    "    return 0  # 无泄露\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Checking test set for potential leakage with multiprocessing...\")\n",
    "    \n",
    "    num_cpus = cpu_count()\n",
    "    print(f\"Using {num_cpus} CPUs\")\n",
    "\n",
    "    with Pool(processes=num_cpus) as pool:\n",
    "        leak_flags = list(tqdm(pool.imap(check_passage_leak, passage_list_dev), total=len(passage_list_dev)))\n",
    "\n",
    "    count_leak = sum(leak_flags)\n",
    "    total_dev = len(passage_list_dev)\n",
    "    leak_percent = count_leak / total_dev * 100\n",
    "\n",
    "    print(f\"\\n测试集泄露条数：{count_leak} / {total_dev}\")\n",
    "    print(f\"测试集泄露率：{leak_percent:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e64ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSH index for dev set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afe6c2008fc4062924cafc8801d7c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for similar dev samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191767d40ebd46229822d035d049cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev集中相似度 >= 0.85 的样本对数量: 10373643\n",
      "\n",
      "[Sample 0] <-> [Sample 6763]  相似度: 1.0000\n",
      "\n",
      "--- Sample 0 ---\n",
      "Xawery Żuławski (born 22 December 1971 in Warsaw) is a Polish film director.\n",
      "In 1995 he graduated National Film School in Łódź.\n",
      "He is the son of actress Małgorzata Braunek and director Andrzej Żuławski.\n",
      "His second feature \"Wojna polsko-ruska\" (2009), adapted from the controversial best-selling novel by Dorota Masłowska, won First Prize in the New Polish Films competition at the 9th Era New Horizons Film Festival in Wrocław.\n",
      "In 2013, he stated he intends to direct a Polish novel \"Zły\" by Leopold Tyrmand.\n",
      "Żuławski and his wife Maria Strzelecka had 2 children together:\n",
      "son Kaj Żuławski (born 2002) and daughter Jagna Żuławska (born 2009).\n",
      "--- Sample 6763 ---\n",
      "Xawery Żuławski (born 22 December 1971 in Warsaw) is a Polish film director.\n",
      "In 1995 he graduated National Film School in Łódź.\n",
      "He is the son of actress Małgorzata Braunek and director Andrzej Żuławski.\n",
      "His second feature \"Wojna polsko-ruska\" (2009), adapted from the controversial best-selling novel by Dorota Masłowska, won First Prize in the New Polish Films competition at the 9th Era New Horizons Film Festival in Wrocław.\n",
      "In 2013, he stated he intends to direct a Polish novel \"Zły\" by Leopold Tyrmand.\n",
      "Żuławski and his wife Maria Strzelecka had 2 children together:\n",
      "son Kaj Żuławski (born 2002) and daughter Jagna Żuławska (born 2009).\n",
      "==============================\n",
      "\n",
      "[Sample 1] <-> [Sample 6761]  相似度: 1.0000\n",
      "\n",
      "--- Sample 1 ---\n",
      "Snow White and the Seven Dwarfs( USA:\" Snow White\") is a 1955 German film, directed by Erich Kobler, based on the story of Schneewittchen by the Brothers Grimm.\n",
      "--- Sample 6761 ---\n",
      "Snow White and the Seven Dwarfs( USA:\" Snow White\") is a 1955 German film, directed by Erich Kobler, based on the story of Schneewittchen by the Brothers Grimm.\n",
      "==============================\n",
      "\n",
      "[Sample 2] <-> [Sample 6766]  相似度: 1.0000\n",
      "\n",
      "--- Sample 2 ---\n",
      "Maheen Khan is a Pakistani fashion and costume designer, also an award winner fashion designer for fashion labels like\" The Embroidery HouseMaheen\" and\" Gulabo\".\n",
      "She has done many national and international fashion events and shows.\n",
      "She undertook embroidery for the film Snow White and the Huntsman and television series\n",
      "The Jewel in the Crown.\n",
      "--- Sample 6766 ---\n",
      "Maheen Khan is a Pakistani fashion and costume designer, also an award winner fashion designer for fashion labels like\" The Embroidery HouseMaheen\" and\" Gulabo\".\n",
      "She has done many national and international fashion events and shows.\n",
      "She undertook embroidery for the film Snow White and the Huntsman and television series\n",
      "The Jewel in the Crown.\n",
      "==============================\n",
      "\n",
      "[Sample 3] <-> [Sample 6769]  相似度: 1.0000\n",
      "\n",
      "--- Sample 3 ---\n",
      "A Snow White Christmas is a Christmas animated television special produced by Filmation and telecast December 19, 1980, on CBS.\n",
      "It is a sequel to the fairy tale\" Snow White\", unrelated to Filmation's other sequel to\" Snow White\" titled\" Happily Ever After\"( 1990).\n",
      "The film's plot revolves around the return of the Wicked Queen, who is unexpectedly brought back to life during Christmas and casts an evil spell that freezes the entire land.\n",
      "Only the young Snow White, the daughter of the original Snow White, manages to escape and take refuge with the seven giants with her dwarf friend.\n",
      "It is now up to the giants to defeat the Queen forever and save the kingdom.\n",
      "--- Sample 6769 ---\n",
      "A Snow White Christmas is a Christmas animated television special produced by Filmation and telecast December 19, 1980, on CBS.\n",
      "It is a sequel to the fairy tale\" Snow White\", unrelated to Filmation's other sequel to\" Snow White\" titled\" Happily Ever After\"( 1990).\n",
      "The film's plot revolves around the return of the Wicked Queen, who is unexpectedly brought back to life during Christmas and casts an evil spell that freezes the entire land.\n",
      "Only the young Snow White, the daughter of the original Snow White, manages to escape and take refuge with the seven giants with her dwarf friend.\n",
      "It is now up to the giants to defeat the Queen forever and save the kingdom.\n",
      "==============================\n",
      "\n",
      "[Sample 4] <-> [Sample 6768]  相似度: 1.0000\n",
      "\n",
      "--- Sample 4 ---\n",
      "Alice Washburn( 1860- 1929) was an American stage and film actress.\n",
      "She worked at the Edison, Vitagraph and Kalem studios.\n",
      "Her final film Snow White was her only known feature film.\n",
      "She died of heart attack in November 1929.\n",
      "--- Sample 6768 ---\n",
      "Alice Washburn( 1860- 1929) was an American stage and film actress.\n",
      "She worked at the Edison, Vitagraph and Kalem studios.\n",
      "Her final film Snow White was her only known feature film.\n",
      "She died of heart attack in November 1929.\n",
      "==============================\n",
      "\n",
      "[Sample 5] <-> [Sample 6762]  相似度: 1.0000\n",
      "\n",
      "--- Sample 5 ---\n",
      "Polish-Russian War\n",
      "(Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish-Russian War under the white-red flag by Dorota Masłowska.\n",
      "--- Sample 6762 ---\n",
      "Polish-Russian War\n",
      "(Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish-Russian War under the white-red flag by Dorota Masłowska.\n",
      "==============================\n",
      "\n",
      "[Sample 6] <-> [Sample 6764]  相似度: 1.0000\n",
      "\n",
      "--- Sample 6 ---\n",
      "Viktor Petrovich Yeliseyev( born June 9, 1950) is a Russian general, orchestra conductor and music teacher.\n",
      "He is the director of the Ministry of the Interior Ensemble, one of the two Russian Red Army Choirs.\n",
      "--- Sample 6764 ---\n",
      "Viktor Petrovich Yeliseyev( born June 9, 1950) is a Russian general, orchestra conductor and music teacher.\n",
      "He is the director of the Ministry of the Interior Ensemble, one of the two Russian Red Army Choirs.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 33742]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 33742 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 96216]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 96216 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 67757]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 67757 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 14400]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 14400 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 55290]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 55290 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 66853]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 66853 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 50690]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 50690 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 106041]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 106041 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 120301]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 120301 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 59227]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 59227 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 90106]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 90106 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 99352]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 99352 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "[Sample 7] <-> [Sample 62699]  相似度: 1.0000\n",
      "\n",
      "--- Sample 7 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "--- Sample 62699 ---\n",
      "She was the mother of Prince Morinaga.\n",
      "==============================\n",
      "\n",
      "总共 83436 条测试样本参与了相似对，占测试集的 66.35%\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "# from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "passage_list = []\n",
    "for data in dataset:\n",
    "    for passages in data['context']:\n",
    "        passages_str = \"\\n\".join(passages[1])\n",
    "        passage_list.append(passages_str)\n",
    "passage_list_dev = []\n",
    "for data in test_dataset:\n",
    "    for passages in data['context']:\n",
    "        passages_str = \"\\n\".join(passages[1])\n",
    "        passage_list_dev.append(passages_str)\n",
    "def get_minhash(text, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "# ========== 2. 建立 Dev 集索引 ==========\n",
    "print(\"Building LSH index for dev set...\")\n",
    "lsh_dev = MinHashLSH(threshold=0.8, num_perm=128)\n",
    "minhash_dev_dict = {}\n",
    "\n",
    "for idx, passage in tqdm(enumerate(passage_list_dev), total=len(passage_list_dev)):\n",
    "    m = get_minhash(passage)\n",
    "    key = f\"dev_{idx}\"\n",
    "    lsh_dev.insert(key, m)\n",
    "    minhash_dev_dict[key] = passage\n",
    "\n",
    "# ========== 3. 查找 dev 内部相似文本对 ==========\n",
    "similar_pairs = []\n",
    "visited_pairs = set()\n",
    "\n",
    "print(\"Searching for similar dev samples...\")\n",
    "for idx, passage in tqdm(enumerate(passage_list_dev), total=len(passage_list_dev)):\n",
    "    key = f\"dev_{idx}\"\n",
    "    m = get_minhash(passage)\n",
    "    candidates = lsh_dev.query(m)\n",
    "\n",
    "    for ckey in candidates:\n",
    "        if ckey == key:\n",
    "            continue  # 跳过自己\n",
    "        j = int(ckey.split('_')[1])\n",
    "        if j <= idx:\n",
    "            continue  # 避免重复统计相同pair\n",
    "\n",
    "        pair_id = (idx, j)\n",
    "        if pair_id in visited_pairs:\n",
    "            continue\n",
    "\n",
    "        candidate_passage = minhash_dev_dict[ckey]\n",
    "        sim = Levenshtein.ratio(passage, candidate_passage)\n",
    "        if sim >= 0.85:\n",
    "            similar_pairs.append((idx, j, sim))\n",
    "            visited_pairs.add(pair_id)\n",
    "\n",
    "# ========== 4. 输出相似结果 ==========\n",
    "print(f\"\\nDev集中相似度 >= 0.85 的样本对数量: {len(similar_pairs)}\\n\")\n",
    "for i, j, sim in similar_pairs[:20]:  # 展示前 20 个\n",
    "    print(f\"[Sample {i}] <-> [Sample {j}]  相似度: {sim:.4f}\\n\")\n",
    "    print(\"--- Sample {} ---\\n{}\".format(i, passage_list_dev[i]))\n",
    "    print(\"--- Sample {} ---\\n{}\".format(j, passage_list_dev[j]))\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "# ========== 5. 可选：统计有重复的 dev 样本比例 ==========\n",
    "leaked_dev_ids = set()\n",
    "for i, j, _ in similar_pairs:\n",
    "    leaked_dev_ids.add(i)\n",
    "    leaked_dev_ids.add(j)\n",
    "\n",
    "leak_percent = len(leaked_dev_ids) / len(passage_list_dev) * 100\n",
    "print(f\"总共 {len(leaked_dev_ids)} 条测试样本参与了相似对，占测试集的 {leak_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea4f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Full embedding matrix: mean = -0.0001, std = 0.0206\n",
      "🆕 New token '<|doc_mask|>': mean = 0.0001, std = 0.0205\n",
      "⚠️  New token embedding might be too different. Consider inspecting further.\n",
      "\n",
      "✅ Model and tokenizer with '<|doc_mask|>' saved to: ./models/Llama-3.2-1B-Instruct-Doc_mask\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"./models/Llama-3.2-1B-Instruct\"  # 替换成你的模型\n",
    "save_path = \"./models/Llama-3.2-1B-Instruct-Doc_mask\"\n",
    "\n",
    "#加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def init_and_save_tokenizer_embedding(llm_model, tokenizer, doc_mask_token, save_path):\n",
    "    # 设置 padding\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # 添加新 token\n",
    "    tokenizer.add_tokens([doc_mask_token], special_tokens=True)\n",
    "    llm_model.resize_token_embeddings(len(tokenizer))  # 自动初始化新 embedding\n",
    "\n",
    "    # 获取 embedding 权重\n",
    "    token_embeddings = llm_model.get_input_embeddings().weight  # [vocab_size, hidden_dim]\n",
    "    print(f\"📊 Full embedding matrix: shape = {token_embeddings.shape}\")\n",
    "\n",
    "    # 检查统计分布是否一致（使用全部 embedding）\n",
    "    ref_mean = token_embeddings.mean().item()\n",
    "    ref_std = token_embeddings.std().item()\n",
    "\n",
    "    token_id = tokenizer.convert_tokens_to_ids(doc_mask_token)\n",
    "\n",
    "    new_token_weight = token_embeddings[token_id]\n",
    "\n",
    "    print(f\"new token original mean: {new_token_weight.mean().item()}, std: {new_token_weight.std().item()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean = token_embeddings.mean(dim=0)\n",
    "        std = token_embeddings.std(dim=0)\n",
    "        noise = torch.randn_like(mean) * std\n",
    "        token_embeddings[token_id] = mean + noise\n",
    "\n",
    "    new_emb = token_embeddings[token_id]\n",
    "    new_mean = new_emb.mean().item()\n",
    "    new_std = new_emb.std().item()\n",
    "\n",
    "    print(f\"\\n📊 Full embedding matrix: mean = {ref_mean:.4f}, std = {ref_std:.4f}\")\n",
    "    print(f\"🆕 New token '{doc_mask_token}': mean = {new_mean:.4f}, std = {new_std:.4f}\")\n",
    "\n",
    "    if not (torch.isfinite(new_emb).all() and torch.isfinite(token_embeddings).all()):\n",
    "        print(\"⚠️  Embedding contains NaNs or Infs. Check upstream initialization.\")\n",
    "    else:\n",
    "        mean_diff = abs(new_mean - ref_mean) / (abs(ref_mean) + 1e-6)\n",
    "        std_diff = abs(new_std - ref_std) / (ref_std + 1e-6)\n",
    "\n",
    "        if mean_diff < 0.1 and std_diff < 0.1:\n",
    "            print(\"✅ New token embedding looks statistically aligned with full embedding set.\\n\")\n",
    "        else:\n",
    "            print(\"⚠️  New token embedding might be too different. Consider inspecting further.\\n\")\n",
    "\n",
    "    # 保存所有组件\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # llm_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    # if hasattr(llm_model, \"generation_config\"):\n",
    "    #     llm_model.generation_config.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"✅ Model and tokenizer with '{doc_mask_token}' saved to: {save_path}\")\n",
    "init_and_save_tokenizer_embedding(model, tokenizer, \"<|doc_mask|>\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb089ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d95c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage', 'question', 'answer', 'full_answer'],\n",
      "    num_rows: 38368\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f751998228f429889b1b66825392ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/38368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "dataset_paths=[\n",
    "    \"./data_aug_deepseek-v3/2wikimultihopqa/bridge_comparison.json\",\n",
    "    \"./data_aug_deepseek-v3/2wikimultihopqa/comparison.json\",\n",
    "    \"./data_aug_deepseek-v3/2wikimultihopqa/compositional.json\",\n",
    "    \"./data_aug_deepseek-v3/2wikimultihopqa/inference.json\",\n",
    "    \"./data_aug_deepseek-v3/complexwebquestions/total.json\",\n",
    "    \"./data_aug_deepseek-v3/hotpotqa/bridge.json\",\n",
    "    \"./data_aug_deepseek-v3/hotpotqa/comparison.json\",\n",
    "    \"./data_aug_deepseek-v3/popqa/total.json\"\n",
    "]\n",
    "# for dataset_path in dataset_paths:\n",
    "#     dataset = json.loads(open(dataset_path, \"r\").read())    \n",
    "#     for data in dataset:\n",
    "#         for augment in data['augment']:\n",
    "#             if augment['ds3_qa_original_output']==\"\":\n",
    "#                 print(\"dataset:\",dataset_path)\n",
    "#                 print(\"test_id:\",data['test_id'])\n",
    "#                 continue\n",
    "#             if augment['deepseekv3_qa']==[]:\n",
    "#                 print(\"dataset:\",dataset_path)\n",
    "#                 print(\"test_id:\",data['test_id'])\n",
    "#                 continue\n",
    "def traindataset_from_jsons(dataset_paths, train_dataset_path):\n",
    "    passages=[]\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    full_answers=[]\n",
    "    for dataset_path in dataset_paths:\n",
    "        dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "        for data in dataset:\n",
    "            for augment in data['augment']:\n",
    "                if augment['deepseekv3_rewrite']==\"\":\n",
    "                    continue\n",
    "                if augment['deepseekv3_qa']==[]:\n",
    "                    continue\n",
    "                qa_cnt = (len(augment['deepseekv3_qa']) + 1) // 2\n",
    "                for idx,qa in enumerate(augment['deepseekv3_qa']):\n",
    "                    if idx < qa_cnt:\n",
    "                        passages.append(augment['passage'])\n",
    "                        questions.append(qa['question'])\n",
    "                        answers.append(qa['answer'])\n",
    "                        full_answers.append(qa['full_answer'])\n",
    "                    passages.append(augment['deepseekv3_rewrite'])\n",
    "                    questions.append(qa['question'])\n",
    "                    answers.append(qa['answer'])\n",
    "                    full_answers.append(qa['full_answer'])\n",
    "    dataset = Dataset.from_dict({\"passage\": passages, \"question\": questions, \"answer\": answers, \"full_answer\": full_answers})\n",
    "    print(dataset)\n",
    "    dataset.save_to_disk(train_dataset_path)\n",
    "train_dataset_path = \"./data_aug_deepseek-v3/train\"\n",
    "traindataset_from_jsons(dataset_paths, train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSH index and deduplicating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1674540/1674540 [33:44<00:00, 826.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始句子数：1674540\n",
      "去重后句子数：366016\n",
      "\n",
      "✅ 去重完成，结果已保存至：data/2wikimultihopqa/train_passages_deduplication.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ========== 1. 加载数据 ==========\n",
    "dataset_path = \"data/2wikimultihopqa/train_passages.json\"\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# 展平为句子列表\n",
    "all_sentences = []\n",
    "for entry in raw_data:\n",
    "    all_sentences.extend(entry[\"passages\"])  # 每条是一个句子字符串\n",
    "\n",
    "# ========== 2. 定义 MinHash 工具 ==========\n",
    "def get_minhash(text, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode(\"utf8\"))\n",
    "    return m\n",
    "\n",
    "# ========== 3. 去重逻辑（LSH + Levenshtein） ==========\n",
    "print(\"Building LSH index and deduplicating...\")\n",
    "\n",
    "lsh = MinHashLSH(threshold=0.8, num_perm=128)\n",
    "minhash_dict = {}\n",
    "unique_sentences = []\n",
    "\n",
    "for idx, sentence in tqdm(enumerate(all_sentences), total=len(all_sentences)):\n",
    "    m = get_minhash(sentence)\n",
    "    candidates = lsh.query(m)\n",
    "\n",
    "    is_duplicate = False\n",
    "    for candidate_key in candidates:\n",
    "        candidate_sentence = minhash_dict[candidate_key]\n",
    "        similarity = Levenshtein.ratio(sentence, candidate_sentence)\n",
    "        if similarity >= 0.85:\n",
    "            is_duplicate = True\n",
    "            break\n",
    "\n",
    "    if not is_duplicate:\n",
    "        key = f\"sent_{len(unique_sentences)}\"\n",
    "        lsh.insert(key, m)\n",
    "        minhash_dict[key] = sentence\n",
    "        unique_sentences.append(sentence)\n",
    "\n",
    "print(f\"原始句子数：{len(all_sentences)}\")\n",
    "print(f\"去重后句子数：{len(unique_sentences)}\")\n",
    "\n",
    "# ========== 4. 保存为原始格式（每条含1句） ==========\n",
    "dedup_data = [{\"passages\": [sent]} for sent in unique_sentences]\n",
    "random.shuffle(dedup_data)\n",
    "output_path = \"data/2wikimultihopqa/train_passages_deduplication.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dedup_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ 去重完成，结果已保存至：{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411f6ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_30000_40000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_40000_50000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_50000_60000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_60000_70000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_70000_80000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_80000_90000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_90000_100000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_100000_110000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_110000_120000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_120000_130000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_130000_140000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_140000_150000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_150000_160000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_160000_170000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_170000_180000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_180000_190000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_190000_200000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_200000_210000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_210000_220000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_220000_230000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_230000_240000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_240000_250000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_250000_260000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_260000_270000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_270000_280000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_280000_290000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_290000_300000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_300000_310000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_310000_320000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_320000_330000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_330000_340000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_340000_350000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_350000_360000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_360000_370000.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_path = \"data/2wikimultihopqa/train_passages_deduplication.json\"\n",
    "dataset_base_dir = \"data/2wikimultihopqa\"\n",
    "datatype=\"train_passages_deduplication\"\n",
    "dataset = json.loads(open(output_path, \"r\").read())\n",
    "FROM = 30000\n",
    "step = 10000\n",
    "for i in range(FROM, len(dataset), step):\n",
    "    dataset_split = dataset[i:i+step]\n",
    "    output_path_split = f\"{dataset_base_dir}/{datatype}_{i}_{i+step}.json\"\n",
    "    with open(output_path_split, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset_split, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"保存数据到 {output_path_split}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb452d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_0_5000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_5000_10000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_10000_15000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_15000_20000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_20000_25000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_25000_30000.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_path = \"data/2wikimultihopqa/train_passages_deduplication.json\"\n",
    "dataset = json.loads(open(output_path, \"r\").read())\n",
    "split = 5000\n",
    "max_num=30000\n",
    "for i in range(0, max_num, split):\n",
    "    dataset_split = dataset[i:i+split]\n",
    "    output_path_split = output_path.replace(\".json\", f\"_{i}_{i+split}.json\")\n",
    "    with open(output_path_split, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset_split, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"保存数据到 {output_path_split}\")\n",
    "# with open(output_path.replace(\".json\",\"_30000.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(dataset[:30000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864160b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_0_300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_300_600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_600_900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_900_1200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_1200_1500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_1500_1800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_1800_2100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_2100_2400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_2400_2700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_2700_3000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_3000_3300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_3300_3600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_3600_3900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_3900_4200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_4200_4500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_4500_4800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_4800_5100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_5100_5400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_5400_5700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_5700_6000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_6000_6300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_6300_6600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_6600_6900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_6900_7200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_7200_7500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_7500_7800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_7800_8100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_8100_8400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_8400_8700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_8700_9000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_9000_9300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_9300_9600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_9600_9900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_9900_10200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_10200_10500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_10500_10800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_10800_11100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_11100_11400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_11400_11700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_11700_12000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_12000_12300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_12300_12600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_12600_12900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_12900_13200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_13200_13500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_13500_13800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_13800_14100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_14100_14400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_14400_14700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_14700_15000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_15000_15300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_15300_15600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_15600_15900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_15900_16200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_16200_16500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_16500_16800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_16800_17100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_17100_17400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_17400_17700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_17700_18000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_18000_18300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_18300_18600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_18600_18900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_18900_19200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_19200_19500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_19500_19800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_19800_20100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_20100_20400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_20400_20700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_20700_21000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_21000_21300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_21300_21600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_21600_21900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_21900_22200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_22200_22500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_22500_22800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_22800_23100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_23100_23400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_23400_23700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_23700_24000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_24000_24300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_24300_24600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_24600_24900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_24900_25200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_25200_25500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_25500_25800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_25800_26100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_26100_26400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_26400_26700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_26700_27000.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_27000_27300.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_27300_27600.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_27600_27900.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_27900_28200.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_28200_28500.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_28500_28800.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_28800_29100.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_29100_29400.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_29400_29700.json\n",
      "保存数据到 data/2wikimultihopqa/train_passages_deduplication_29700_30000.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "# base_path = \"data/2wikimultihopqa/train_passages_deduplication_0_30000.json\"\n",
    "for i in range(0, 30000, 300):\n",
    "    dataset_path = f\"data/2wikimultihopqa/train_passages_deduplication_{i}_{i+300}.json\"\n",
    "    dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "    for idx, data in enumerate(dataset):\n",
    "        if idx == len(dataset) - 1:\n",
    "            break\n",
    "        data['passages'].append(dataset[idx+1]['passages'][0])\n",
    "    with open(dataset_path.replace(\"passages\",\"2passages\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"保存数据到 {dataset_path.replace(\"passages\",\"2passages\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b3c52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_0_300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_300_600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_600_900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_900_1200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_1200_1500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_1500_1800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_1800_2100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_2100_2400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_2400_2700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_2700_3000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_3000_3300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_3300_3600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_3600_3900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_3900_4200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_4200_4500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_4500_4800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_4800_5100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_5100_5400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_5400_5700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_5700_6000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_6000_6300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_6300_6600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_6600_6900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_6900_7200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_7200_7500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_7500_7800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_7800_8100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_8100_8400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_8400_8700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_8700_9000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_9000_9300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_9300_9600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_9600_9900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_9900_10200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_10200_10500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_10500_10800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_10800_11100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_11100_11400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_11400_11700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_11700_12000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_12000_12300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_12300_12600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_12600_12900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_12900_13200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_13200_13500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_13500_13800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_13800_14100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_14100_14400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_14400_14700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_14700_15000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_15000_15300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_15300_15600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_15600_15900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_15900_16200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_16200_16500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_16500_16800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_16800_17100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_17100_17400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_17400_17700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_17700_18000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_18000_18300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_18300_18600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_18600_18900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_18900_19200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_19200_19500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_19500_19800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_19800_20100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_20100_20400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_20400_20700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_20700_21000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_21000_21300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_21300_21600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_21600_21900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_21900_22200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_22200_22500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_22500_22800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_22800_23100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_23100_23400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_23400_23700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_23700_24000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_24000_24300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_24300_24600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_24600_24900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_24900_25200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_25200_25500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_25500_25800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_25800_26100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_26100_26400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_26400_26700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_26700_27000.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_27000_27300.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_27300_27600.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_27600_27900.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_27900_28200.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_28200_28500.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_28500_28800.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_28800_29100.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_29100_29400.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_29400_29700.json\n",
      "保存数据到 data/2wikimultihopqa/train_3passages_deduplication_29700_30000.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "# base_path = \"data/2wikimultihopqa/train_passages_deduplication_0_30000.json\"\n",
    "for i in range(0, 30000, 300):\n",
    "    dataset_path = f\"data/2wikimultihopqa/train_passages_deduplication_{i}_{i+300}.json\"\n",
    "    dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "    for idx, data in enumerate(dataset):\n",
    "        if idx == len(dataset) - 2:\n",
    "            break\n",
    "        data['passages'].append(dataset[idx+1]['passages'][0])\n",
    "        data['passages'].append(dataset[idx+2]['passages'][0])\n",
    "    with open(dataset_path.replace(\"passages\",\"3passages\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"保存数据到 {dataset_path.replace('passages','3passages')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0f0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage', 'question', 'answer', 'full_answer'],\n",
      "    num_rows: 134893\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33e8465c5bf4bc9b9de2d370ff0190c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/134893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage :  Passage:\n",
      "Fíaskó:\n",
      "Fíaskó is a 2021 Icelandic drama film directed by Ragnar Bragason.\n",
      "\n",
      "question :  What is the title of the Icelandic film?\n",
      "answer :  Fíaskó\n",
      "full_answer :  Fíaskó is an Icelandic film written and directed by Ragnar Bragason.\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "dataset_path_base=\"data_aug_Llama-3.2-1B-Instruct-Doc_mask/2wikimultihopqa\"\n",
    "datatype = \"train_passages_deduplication\"\n",
    "preffix = \"Llama-3.2-1B-Instruct-Doc_mask\"\n",
    "passage_list=[]\n",
    "answer_list=[]\n",
    "question_list=[]\n",
    "full_answer_list=[]\n",
    "step=5000\n",
    "dataset_names=[\n",
    "    \"train_passages_deduplication_0_30000.json\",\n",
    "    \"train_passages_deduplication_30000_40000.json\",\n",
    "    \"train_passages_deduplication_40000_50000.json\",\n",
    "    \"train_passages_deduplication_50000_60000.json\",\n",
    "    \"train_passages_deduplication_60000_70000.json\",\n",
    "    \"train_passages_deduplication_70000_80000.json\",\n",
    "    \"train_passages_deduplication_80000_90000.json\",\n",
    "]\n",
    "# for i in range(0, 30000, step):\n",
    "#     dataset_name = f\"{datatype}_{i}_{i+step}.json\"\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_path = os.path.join(dataset_path_base, dataset_name)\n",
    "    dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "    for data in dataset:\n",
    "        if \"augment\" not in data:\n",
    "            continue\n",
    "        for augment in data['augment']:\n",
    "            if augment[f'{preffix}_rewrite']==\"\" or augment[f'{preffix}_rewrite']==[]:\n",
    "                continue\n",
    "            if isinstance(augment[f'{preffix}_rewrite'], list) and \"\" in augment[f'{preffix}_rewrite']:\n",
    "                continue\n",
    "            if augment[f'{preffix}_qa']==[]:\n",
    "                continue\n",
    "            passage=\"\"\n",
    "            if isinstance(augment[f'{preffix}_rewrite'], list):\n",
    "                for idx,passage_ in enumerate(augment[f'{preffix}_rewrite']):\n",
    "                    passage+=f\"Passage {idx+1}:\\n{passage_}\\n\"  \n",
    "            elif isinstance(augment[f'{preffix}_rewrite'], str):\n",
    "                passage+=f\"Passage:\\n{augment[f'{preffix}_rewrite']}\\n\"\n",
    "\n",
    "            for qa in augment[f'{preffix}_qa']:\n",
    "                passage_list.append(passage)\n",
    "                question_list.append(qa['question'])\n",
    "                if isinstance(qa['answer'], list):\n",
    "                    answer_list.append(\" \".join(qa['answer']))\n",
    "                elif isinstance(qa['answer'], str):\n",
    "                    answer_list.append(qa['answer'])\n",
    "                else:\n",
    "                    answer_list.append(str(qa['answer']))\n",
    "                full_answer_list.append(qa['full_answer'])\n",
    "\n",
    "dataset = Dataset.from_dict({\"passage\": passage_list, \"question\": question_list, \"answer\": answer_list, \"full_answer\": full_answer_list})\n",
    "print(dataset)\n",
    "dataset.save_to_disk(f\"{dataset_path_base}/{datatype}_0_90000\")\n",
    "for key in dataset[0].keys():\n",
    "    print(key,\": \" , dataset[0][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50efb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "保存数据到 ./data_aug_Llama-3.2-1B-Instruct-Doc_mask/2wikimultihopqa/train_passages_deduplication_0_30000.json 成功！\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "dataset_path_base=\"./data_aug_Llama-3.2-1B-Instruct-Doc_mask/2wikimultihopqa\"\n",
    "datatype=\"train_passages_deduplication\"\n",
    "data_list=[]\n",
    "step=5000\n",
    "for i in range(0, 30000, step):\n",
    "    dataset_name = f\"{datatype}_{i}_{i+step}.json\"\n",
    "    dataset_path = os.path.join(dataset_path_base, dataset_name)\n",
    "    dataset = json.loads(open(dataset_path, \"r\").read())\n",
    "    data_list.extend(dataset)\n",
    "print(len(data_list))\n",
    "with open(f\"{dataset_path_base}/{datatype}_0_30000.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"保存数据到 {dataset_path_base}/{datatype}_0_30000.json 成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b7191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage :  Swamp Creek is a waterway located in Preble County, Ohio, United States.  \n",
      "The stream derives its name from the wetland areas along its upper reaches.\n",
      "question :  In which country is Swamp Creek situated?\n",
      "answer :  United States\n",
      "full_answer :  Swamp Creek is situated in the United States.\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "dataset_path='./data_aug_deepseek-v3/2wikimultihopqa/train_passages_deduplication_0_4800'\n",
    "dataset = Dataset.load_from_disk(dataset_path)\n",
    "idx = random.randint(0, len(dataset))\n",
    "for key in dataset[idx]:\n",
    "    print(key,\": \" , dataset[idx][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffd76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 983,040 || all params: 1,236,797,440 || trainable%: 0.0795\n",
      "Loss: 3.2622\n",
      "Loss: 0.7738\n",
      "Loss: 2.0324\n",
      "Loss: 0.8017\n",
      "Loss: 2.6171\n",
      "Loss: 1.7202\n",
      "Loss: 1.4711\n",
      "Loss: 0.2446\n",
      "Loss: 1.2898\n",
      "Loss: 0.1459\n",
      "Loss: 1.1576\n",
      "Loss: 1.2492\n",
      "Loss: 0.7548\n",
      "Loss: 0.7476\n",
      "Loss: 0.0702\n",
      "Loss: 0.7110\n",
      "Loss: 0.4445\n",
      "Loss: 0.0904\n",
      "Loss: 0.1447\n",
      "Loss: 0.3452\n",
      "Loss: 0.4771\n",
      "Loss: 0.0106\n",
      "Loss: 0.0602\n",
      "Loss: 0.3635\n",
      "Loss: 0.3030\n",
      "Loss: 0.0185\n",
      "Loss: 0.0927\n",
      "Loss: 0.1765\n",
      "Loss: 0.0034\n",
      "Loss: 0.0179\n",
      "Loss: 0.0137\n",
      "Loss: 0.0506\n",
      "Loss: 0.0551\n",
      "Loss: 0.0017\n",
      "Loss: 0.0021\n",
      "Loss: 0.0427\n",
      "Loss: 0.0313\n",
      "Loss: 0.0012\n",
      "Loss: 0.0044\n",
      "Loss: 0.0013\n",
      "Loss: 0.0068\n",
      "Loss: 0.0259\n",
      "Loss: 0.0028\n",
      "Loss: 0.0098\n",
      "Loss: 0.0175\n",
      "Loss: 0.0033\n",
      "Loss: 0.0008\n",
      "Loss: 0.0005\n",
      "Loss: 0.0046\n",
      "Loss: 0.0004\n",
      "Loss: 0.0007\n",
      "Loss: 0.0061\n",
      "Loss: 0.0014\n",
      "Loss: 0.0009\n",
      "Loss: 0.0024\n",
      "Loss: 0.0011\n",
      "Loss: 0.0035\n",
      "Loss: 0.0005\n",
      "Loss: 0.0007\n",
      "Loss: 0.0002\n",
      "Adapter saved to ./temp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "\n",
    "# ==== 自定义两部电影 ====\n",
    "data = [\n",
    "    {\n",
    "        \"title\": \"The Creator (2023)\",\n",
    "        \"summary\": \"In a future war between humans and AI, an ex-special forces agent is tasked with killing the Creator, an AI architect who has developed a mysterious weapon with the power to end the war.\",\n",
    "        \"qa\": [\n",
    "            {\"question\": \"Who is the main antagonist in The Creator?\", \"answer\": \"The Creator, an advanced AI architect.\"},\n",
    "            {\"question\": \"What mission is the agent sent on?\", \"answer\": \"To kill the Creator and destroy the AI weapon.\"},\n",
    "            {\"question\": \"What is the central theme of The Creator?\", \"answer\": \"The conflict between humans and artificial intelligence.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Poor Things (2023)\",\n",
    "        \"summary\": \"Bella Baxter, a woman brought back to life by an unorthodox scientist, sets out on a journey of self-discovery and liberation across Victorian society.\",\n",
    "        \"qa\": [\n",
    "            {\"question\": \"Who revives Bella Baxter?\", \"answer\": \"An unorthodox scientist.\"},\n",
    "            {\"question\": \"What journey does Bella undertake?\", \"answer\": \"A journey of self-discovery and liberation.\"},\n",
    "            {\"question\": \"What is the setting of Poor Things?\", \"answer\": \"Victorian-era society.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# ==== Prompt 模板 ====\n",
    "def make_prompt(summary, question, answer):\n",
    "    return f\"\"\"You should answer the question based on the passage below.\n",
    "\n",
    "Passage: {summary}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\"\"\"\n",
    "def get_chat_tokens(tokenizer, user_msg, assistant_msg, max_len=512):\n",
    "    # 构造 full prompt（user + assistant），加上 EOS\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "    ]\n",
    "    full = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True\n",
    "    )\n",
    "    full_input_ids = full[\"input_ids\"].squeeze(0)\n",
    "    full_attention_mask = full[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    # 获取 assistant 开始位置（通过构造仅 user 的 token）\n",
    "    user_only_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": user_msg}],\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).squeeze(0)\n",
    "    assistant_start = user_only_ids.shape[0]\n",
    "\n",
    "    # 构造 labels\n",
    "    labels = full_input_ids.clone()\n",
    "    labels[:assistant_start] = -100\n",
    "    labels[full_attention_mask == 0] = -100\n",
    "\n",
    "    return full_input_ids, full_attention_mask, labels\n",
    "\n",
    "# ==== 构造训练数据 ====\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_len=512):\n",
    "        self.samples = []\n",
    "        for entry in data:\n",
    "            summary = entry[\"summary\"]\n",
    "            for qa in entry[\"qa\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answer = qa[\"answer\"]\n",
    "                user_msg = f\"You should answer the question based on the passage below.\\n\\nPassage: {summary}\\n\\nQuestion: {question}\"\n",
    "                \n",
    "                input_ids, attention_mask, labels = get_chat_tokens(tokenizer, user_msg, answer)\n",
    "                self.samples.append({\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"labels\": labels\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# ==== 模型与训练 ====\n",
    "model_path = \"./models/Llama-3.2-1B-Instruct\"\n",
    "device = \"cuda:7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16,device_map=device)\n",
    "\n",
    "# LoRA 配置\n",
    "peft_config=LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=2,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=['down_proj', 'gate_proj', 'up_proj'],\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "dataset = SimpleDataset(tokenizer, data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ==== 保存 Adapter ====\n",
    "save_dir = \"./temp\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "print(f\"Adapter saved to {save_dir}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e4cca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/TAP/anaconda3/envs/dyprag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt ===\n",
      "Answer the following question.\n",
      "\n",
      "Question: What mission is the agent sent on?\n",
      "Answer:\n",
      "\n",
      "=== 原始模型回答（无 Adapter） ===\n",
      "Answer the following question.\n",
      "\n",
      "Question: What mission is the agent sent on?\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to explore the surface of Mars.\n",
      "\n",
      "Answer: The mission is to\n",
      "\n",
      "=== 加载 LoRA Adapter 后的回答 ===\n",
      "Answer the following question.\n",
      "\n",
      "Question: What mission is the agent sent on?\n",
      "Answer: To explore and understand the universe.\n",
      "\n",
      "Agent: \"Hello, I'm an astronaut on a mission to explore the universe and understand its mysteries.\"\n",
      "\n",
      "Mission: To explore and understand the universe.\n",
      "\n",
      "Agent: \"We're approaching the planet Mars. Prepare for landing.\"\n",
      "\n",
      "Mission: To explore and understand the universe.\n",
      "\n",
      "Agent: \"We've landed on Mars. We're collecting samples and conducting experiments.\"\n",
      "\n",
      "Mission: To explore and understand the universe.\n",
      "\n",
      "Agent: \"We've collected samples and conducted experiments. We're now\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "# ==== 设置模型路径 ====\n",
    "model_path = \"./models/Llama-3.2-1B-Instruct\"\n",
    "adapter_path = \"./temp\"\n",
    "device = 'cuda:7'\n",
    "# ==== 加载 Tokenizer 和基础模型 ====\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=device)\n",
    "base_model.eval()\n",
    "\n",
    "# ==== 用户输入问题 ====\n",
    "question = \"What mission is the agent sent on?\"  # 💡 请在这里填写你的问题\n",
    "assert question.strip() != \"\", \"请先填写一个问题再运行脚本！\"\n",
    "\n",
    "prompt = f\"\"\"Answer the following question.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ==== 原始模型回答 ====\n",
    "with torch.no_grad():\n",
    "    output_base = base_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    answer_base = tokenizer.decode(output_base[0], skip_special_tokens=True)\n",
    "\n",
    "# ==== 加载 Adapter 后模型回答 ====\n",
    "peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "peft_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_adapter = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    answer_adapter = tokenizer.decode(output_adapter[0], skip_special_tokens=True)\n",
    "\n",
    "# ==== 显示结果 ====\n",
    "print(\"\\n=== Prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== 原始模型回答（无 Adapter） ===\")\n",
    "print(answer_base)\n",
    "\n",
    "print(\"\\n=== 加载 LoRA Adapter 后的回答 ===\")\n",
    "print(answer_adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f52945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage', 'question', 'answer', 'full_answer'],\n",
      "    num_rows: 139723\n",
      "})\n",
      "Dataset({\n",
      "    features: ['passage', 'question', 'answer', 'full_answer'],\n",
      "    num_rows: 149356\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "dataset_path1=\"./data_aug_deepseek-v3/2wikimultihopqa/train_passages_deduplication_0_30000\"\n",
    "dataset_path2=\"./data_aug_deepseek-v3/2wikimultihopqa/train_2passages_deduplication_0_30000\"\n",
    "dataset1 = load_from_disk(dataset_path1)\n",
    "dataset2 = load_from_disk(dataset_path2)\n",
    "print(dataset1)\n",
    "print(dataset2)\n",
    "\n",
    "# print(len(dataset1), len(dataset2))\n",
    "# dataset = concatenate_datasets([dataset1, dataset2])\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbf556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbedb4964f94a2aba8c8aba3250dcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/238973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['passage', 'question', 'answer', 'full_answer'],\n",
      "    num_rows: 238973\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "# dataset_path=\"./data_aug_deepseek-v3/2wikimultihopqa/train_passages_deduplication_0_30000.json\"\n",
    "# save_dataset_path=\"./data_aug_deepseek-v3/2wikimultihopqa/train_passages_deduplication_0_30000_v2\"#226720\n",
    "dataset_path=\"./data_aug_deepseek-v3/2wikimultihopqa/train_2passages_deduplication_0_30000.json\"\n",
    "save_dataset_path=\"./data_aug_deepseek-v3/2wikimultihopqa/train_2passages_deduplication_0_30000_v2\"#238973\n",
    "\n",
    "dataset=json.load(open(dataset_path,\"r\"))\n",
    "passages=[]\n",
    "questions=[]\n",
    "answers=[]\n",
    "full_answers=[]\n",
    "for data in dataset:\n",
    "    for augment in data[\"augment\"]:\n",
    "        qa_cnt = (len(augment['deepseekv3_qa'])+1)//2\n",
    "        for idx,qa in enumerate(augment['deepseekv3_qa']):\n",
    "            if idx<qa_cnt:\n",
    "                passages.append(augment[\"passage\"])\n",
    "                questions.append(qa[\"question\"])\n",
    "                answers.append(qa[\"answer\"])\n",
    "                full_answers.append(qa[\"full_answer\"])\n",
    "            if isinstance(augment['deepseekv3_rewrite'],list):\n",
    "                psg = \"\"\n",
    "                for i, p in enumerate(augment['deepseekv3_rewrite']):\n",
    "                    psg+=f\"Passage {i+1}:\\n{p}\\n\"\n",
    "            else:\n",
    "                psg = augment['deepseekv3_rewrite']\n",
    "            passages.append(psg)\n",
    "            questions.append(qa[\"question\"])\n",
    "            answers.append(qa[\"answer\"])\n",
    "            full_answers.append(qa[\"full_answer\"])\n",
    "dataset = Dataset.from_dict({\"passage\": passages, \"question\": questions, \"answer\": answers, \"full_answer\": full_answers})\n",
    "dataset.save_to_disk(save_dataset_path)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa27ace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'base_model.model.model.layers.0.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.0.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9515e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0034e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7400e-04, -4.7662e-04],\n",
      "         [ 6.7981e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.down_proj.lora_A.weight': tensor([[[ 1.2051, -0.7241,  0.4104,  ...,  1.6417, -0.1357, -0.4218],\n",
      "         [-0.2668, -0.7058,  0.9664,  ...,  0.8348, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.down_proj.lora_B.weight': tensor([[[-7.2697e-04,  2.9479e-04],\n",
      "         [ 2.5937e-04, -5.1498e-04],\n",
      "         [ 1.0045e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7392e-04, -4.7676e-04],\n",
      "         [ 6.8000e-05, -7.1924e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8242e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.down_proj.lora_A.weight': tensor([[[ 1.2060, -0.7238,  0.4108,  ...,  1.6423, -0.1321, -0.4198],\n",
      "         [-0.2664, -0.7046,  0.9597,  ...,  0.8334, -1.1130,  1.0804]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3359, -0.4394,  0.0481],\n",
      "         [ 0.1797, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.down_proj.lora_B.weight': tensor([[[-7.2843e-04,  2.9893e-04],\n",
      "         [ 2.5750e-04, -5.1458e-04],\n",
      "         [ 9.8807e-05, -1.0361e-03],\n",
      "         ...,\n",
      "         [-1.2833e-03,  1.2390e-03],\n",
      "         [-1.7440e-04, -4.7462e-04],\n",
      "         [ 6.8132e-05, -7.1967e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4738e-04,  1.8420e-04],\n",
      "         [ 3.1449e-04, -4.8244e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6816e-04],\n",
      "         [-3.1153e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7275e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3137,  ...,  1.3326, -0.0485,  0.0358],\n",
      "         [ 0.3425, -0.7205,  1.5429,  ...,  1.9274, -0.7672,  0.7374]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9513e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0035e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7399e-04, -4.7663e-04],\n",
      "         [ 6.7982e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5738e-04,  1.6219e-04],\n",
      "         [ 6.9987e-04, -2.2474e-04],\n",
      "         [ 4.3066e-04, -9.8227e-04],\n",
      "         ...,\n",
      "         [-2.5885e-04,  6.8966e-04],\n",
      "         [-7.8500e-04,  2.0102e-04],\n",
      "         [ 5.1161e-04, -1.0006e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8242e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.down_proj.lora_A.weight': tensor([[[ 1.2052, -0.7241,  0.4104,  ...,  1.6417, -0.1356, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9662,  ...,  0.8348, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.down_proj.lora_B.weight': tensor([[[-7.2705e-04,  2.9503e-04],\n",
      "         [ 2.5931e-04, -5.1499e-04],\n",
      "         [ 1.0037e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7397e-04, -4.7666e-04],\n",
      "         [ 6.7987e-05, -7.1926e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.down_proj.lora_A.weight': tensor([[[ 1.2052, -0.7231,  0.4107,  ...,  1.6433, -0.1304, -0.4189],\n",
      "         [-0.2657, -0.7041,  0.9576,  ...,  0.8328, -1.1136,  1.0803]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.down_proj.lora_B.weight': tensor([[[-7.2910e-04,  3.0069e-04],\n",
      "         [ 2.5698e-04, -5.1467e-04],\n",
      "         [ 9.8076e-05, -1.0375e-03],\n",
      "         ...,\n",
      "         [-1.2833e-03,  1.2397e-03],\n",
      "         [-1.7500e-04, -4.7322e-04],\n",
      "         [ 6.8647e-05, -7.2016e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9515e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0034e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7400e-04, -4.7662e-04],\n",
      "         [ 6.7981e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8242e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.down_proj.lora_A.weight': tensor([[[ 1.2033, -0.7231,  0.4111,  ...,  1.6445, -0.1300, -0.4186],\n",
      "         [-0.2658, -0.7058,  0.9576,  ...,  0.8326, -1.1143,  1.0827]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3137,  ...,  1.3326, -0.0485,  0.0358],\n",
      "         [ 0.3425, -0.7205,  1.5429,  ...,  1.9274, -0.7672,  0.7374]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.down_proj.lora_B.weight': tensor([[[-7.2850e-04,  2.9912e-04],\n",
      "         [ 2.5782e-04, -5.1561e-04],\n",
      "         [ 9.7567e-05, -1.0376e-03],\n",
      "         ...,\n",
      "         [-1.2819e-03,  1.2390e-03],\n",
      "         [-1.7582e-04, -4.7230e-04],\n",
      "         [ 7.0226e-05, -7.2162e-04]],\n",
      "\n",
      "        [[-5.5738e-04,  1.6219e-04],\n",
      "         [ 6.9987e-04, -2.2474e-04],\n",
      "         [ 4.3066e-04, -9.8227e-04],\n",
      "         ...,\n",
      "         [-2.5885e-04,  6.8966e-04],\n",
      "         [-7.8500e-04,  2.0102e-04],\n",
      "         [ 5.1161e-04, -1.0006e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9515e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0034e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7400e-04, -4.7662e-04],\n",
      "         [ 6.7981e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.down_proj.lora_A.weight': tensor([[[ 1.2033, -0.7231,  0.4111,  ...,  1.6445, -0.1300, -0.4186],\n",
      "         [-0.2658, -0.7058,  0.9576,  ...,  0.8326, -1.1143,  1.0828]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.down_proj.lora_B.weight': tensor([[[-7.2850e-04,  2.9912e-04],\n",
      "         [ 2.5782e-04, -5.1562e-04],\n",
      "         [ 9.7565e-05, -1.0376e-03],\n",
      "         ...,\n",
      "         [-1.2819e-03,  1.2391e-03],\n",
      "         [-1.7583e-04, -4.7229e-04],\n",
      "         [ 7.0233e-05, -7.2163e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2619e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.down_proj.lora_A.weight': tensor([[[ 1.2066, -0.7247,  0.4110,  ...,  1.6409, -0.1343, -0.4208],\n",
      "         [-0.2675, -0.7051,  0.9625,  ...,  0.8345, -1.1127,  1.0806]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3139,  ...,  1.3315, -0.0480,  0.0364],\n",
      "         [ 0.3428, -0.7204,  1.5423,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.down_proj.lora_B.weight': tensor([[[-7.2755e-04,  2.9648e-04],\n",
      "         [ 2.5849e-04, -5.1412e-04],\n",
      "         [ 9.9702e-05, -1.0342e-03],\n",
      "         ...,\n",
      "         [-1.2836e-03,  1.2377e-03],\n",
      "         [-1.7368e-04, -4.7656e-04],\n",
      "         [ 6.7807e-05, -7.1918e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6192e-04],\n",
      "         [ 7.0071e-04, -2.2605e-04],\n",
      "         [ 4.3086e-04, -9.8212e-04],\n",
      "         ...,\n",
      "         [-2.5852e-04,  6.9034e-04],\n",
      "         [-7.8523e-04,  2.0229e-04],\n",
      "         [ 5.1169e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8242e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.down_proj.lora_A.weight': tensor([[[ 1.2033, -0.7231,  0.4111,  ...,  1.6445, -0.1300, -0.4186],\n",
      "         [-0.2658, -0.7058,  0.9576,  ...,  0.8326, -1.1143,  1.0828]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.down_proj.lora_B.weight': tensor([[[-7.2850e-04,  2.9912e-04],\n",
      "         [ 2.5782e-04, -5.1562e-04],\n",
      "         [ 9.7565e-05, -1.0376e-03],\n",
      "         ...,\n",
      "         [-1.2819e-03,  1.2391e-03],\n",
      "         [-1.7582e-04, -4.7229e-04],\n",
      "         [ 7.0232e-05, -7.2163e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2540, -0.5524,  0.1731,  ...,  1.3347, -0.4392,  0.0486],\n",
      "         [ 0.1784, -0.7693,  1.2269,  ...,  1.3005, -0.4948,  0.9435]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9515e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0034e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7400e-04, -4.7662e-04],\n",
      "         [ 6.7981e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4804e-04,  1.8334e-04],\n",
      "         [ 3.1487e-04, -4.8373e-04],\n",
      "         [ 2.1295e-04, -3.6038e-04],\n",
      "         ...,\n",
      "         [-4.1831e-04,  4.6695e-04],\n",
      "         [-3.1191e-04, -4.6631e-04],\n",
      "         [ 1.2103e-04, -6.7242e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.down_proj.lora_A.weight': tensor([[[ 1.2033, -0.7231,  0.4111,  ...,  1.6445, -0.1300, -0.4186],\n",
      "         [-0.2658, -0.7058,  0.9576,  ...,  0.8326, -1.1143,  1.0828]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.down_proj.lora_B.weight': tensor([[[-7.2850e-04,  2.9912e-04],\n",
      "         [ 2.5782e-04, -5.1562e-04],\n",
      "         [ 9.7565e-05, -1.0376e-03],\n",
      "         ...,\n",
      "         [-1.2819e-03,  1.2391e-03],\n",
      "         [-1.7583e-04, -4.7229e-04],\n",
      "         [ 7.0233e-05, -7.2163e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.down_proj.lora_A.weight': tensor([[[ 1.2052, -0.7231,  0.4107,  ...,  1.6433, -0.1304, -0.4189],\n",
      "         [-0.2657, -0.7041,  0.9576,  ...,  0.8328, -1.1136,  1.0803]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.down_proj.lora_B.weight': tensor([[[-7.2909e-04,  3.0068e-04],\n",
      "         [ 2.5698e-04, -5.1467e-04],\n",
      "         [ 9.8080e-05, -1.0375e-03],\n",
      "         ...,\n",
      "         [-1.2833e-03,  1.2397e-03],\n",
      "         [-1.7500e-04, -4.7323e-04],\n",
      "         [ 6.8644e-05, -7.2016e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.down_proj.lora_A.weight': tensor([[[ 1.2053, -0.7240,  0.4104,  ...,  1.6417, -0.1355, -0.4218],\n",
      "         [-0.2668, -0.7057,  0.9661,  ...,  0.8347, -1.1121,  1.0819]],\n",
      "\n",
      "        [[-0.3297, -0.1737, -0.3140,  ...,  1.3314, -0.0479,  0.0365],\n",
      "         [ 0.3429, -0.7204,  1.5422,  ...,  1.9277, -0.7670,  0.7371]],\n",
      "\n",
      "        [[-0.2156,  0.0690, -0.4096,  ...,  1.3450, -0.0708, -0.1658],\n",
      "         [ 0.0184, -0.5834,  1.3559,  ...,  2.1263, -0.0130,  0.4980]],\n",
      "\n",
      "        [[ 0.2532, -0.5518,  0.1721,  ...,  1.3360, -0.4394,  0.0481],\n",
      "         [ 0.1798, -0.7684,  1.2285,  ...,  1.3014, -0.4939,  0.9432]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.down_proj.lora_B.weight': tensor([[[-7.2708e-04,  2.9515e-04],\n",
      "         [ 2.5928e-04, -5.1499e-04],\n",
      "         [ 1.0034e-04, -1.0345e-03],\n",
      "         ...,\n",
      "         [-1.2846e-03,  1.2386e-03],\n",
      "         [-1.7400e-04, -4.7662e-04],\n",
      "         [ 6.7981e-05, -7.1927e-04]],\n",
      "\n",
      "        [[-5.5733e-04,  1.6189e-04],\n",
      "         [ 7.0081e-04, -2.2618e-04],\n",
      "         [ 4.3089e-04, -9.8211e-04],\n",
      "         ...,\n",
      "         [-2.5848e-04,  6.9039e-04],\n",
      "         [-7.8525e-04,  2.0242e-04],\n",
      "         [ 5.1171e-04, -1.0002e-03]],\n",
      "\n",
      "        [[-8.1486e-04,  3.4796e-04],\n",
      "         [ 6.7612e-04, -4.2976e-04],\n",
      "         [ 8.0945e-04, -5.1290e-04],\n",
      "         ...,\n",
      "         [-4.6287e-04,  6.9794e-04],\n",
      "         [-4.7913e-04,  9.9218e-05],\n",
      "         [ 1.5931e-04, -8.4663e-04]],\n",
      "\n",
      "        [[-5.4737e-04,  1.8421e-04],\n",
      "         [ 3.1448e-04, -4.8243e-04],\n",
      "         [ 2.1290e-04, -3.6053e-04],\n",
      "         ...,\n",
      "         [-4.1839e-04,  4.6818e-04],\n",
      "         [-3.1152e-04, -4.6616e-04],\n",
      "         [ 1.2049e-04, -6.7276e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.0.mlp.up_proj.lora_A.weight': tensor([[[-0.1240, -0.0782,  0.5900,  ..., -1.2082,  0.0250, -0.1594],\n",
      "         [ 1.7672, -0.0391, -0.2005,  ..., -0.2534, -0.9640,  0.8592]],\n",
      "\n",
      "        [[-0.1401,  0.6690, -0.4127,  ..., -0.6317,  0.1673, -0.6455],\n",
      "         [ 1.0115,  0.2006, -0.7793,  ..., -0.3995, -1.2708,  0.1378]],\n",
      "\n",
      "        [[-0.3525,  1.0459,  0.7150,  ...,  0.1532,  0.7821, -1.3395],\n",
      "         [ 1.1603,  0.1140,  0.3393,  ..., -0.0063, -1.8525,  0.1316]],\n",
      "\n",
      "        [[-0.6478,  0.8261,  0.2733,  ...,  0.2991,  0.0416, -0.6939],\n",
      "         [ 1.1681,  0.1048,  0.7817,  ..., -0.0606, -1.7541,  1.0871]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.0.mlp.up_proj.lora_B.weight': tensor([[[ 3.9270e-04, -5.5813e-04],\n",
      "         [ 5.9874e-04, -1.6015e-03],\n",
      "         [-6.8561e-04, -6.3532e-04],\n",
      "         ...,\n",
      "         [-4.5998e-04,  3.3729e-04],\n",
      "         [-9.0619e-04,  6.9979e-05],\n",
      "         [-1.0303e-03, -4.9791e-04]],\n",
      "\n",
      "        [[-1.1974e-04, -6.4738e-05],\n",
      "         [ 5.1623e-04, -2.5083e-04],\n",
      "         [-9.3076e-04,  3.1960e-04],\n",
      "         ...,\n",
      "         [-3.5392e-05,  6.9844e-04],\n",
      "         [ 5.4174e-05, -5.2801e-04],\n",
      "         [-3.6650e-04,  6.0851e-05]],\n",
      "\n",
      "        [[-2.3977e-04,  4.4304e-04],\n",
      "         [ 1.5034e-03, -7.1772e-04],\n",
      "         [-9.3177e-04, -3.2198e-04],\n",
      "         ...,\n",
      "         [-1.1175e-04,  3.5271e-04],\n",
      "         [-3.0779e-04, -3.6923e-04],\n",
      "         [-7.0426e-04, -6.8000e-04]],\n",
      "\n",
      "        [[ 3.9060e-04, -6.9979e-04],\n",
      "         [ 1.3601e-03, -6.5913e-04],\n",
      "         [-5.8216e-04, -1.4083e-03],\n",
      "         ...,\n",
      "         [-2.7353e-04,  1.7484e-04],\n",
      "         [-3.2653e-04, -3.6902e-05],\n",
      "         [-1.4225e-03, -2.0017e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.up_proj.lora_A.weight': tensor([[[-0.1263, -0.0962,  0.6411,  ..., -1.2021,  0.0472, -0.1801],\n",
      "         [ 1.7827, -0.0502, -0.1424,  ..., -0.2385, -0.9673,  0.8566]],\n",
      "\n",
      "        [[-0.1390,  0.6660, -0.4102,  ..., -0.6331,  0.1688, -0.6487],\n",
      "         [ 1.0155,  0.1984, -0.7773,  ..., -0.4004, -1.2725,  0.1395]],\n",
      "\n",
      "        [[-0.3542,  1.0443,  0.7200,  ...,  0.1529,  0.7862, -1.3411],\n",
      "         [ 1.1614,  0.1140,  0.3435,  ..., -0.0056, -1.8535,  0.1301]],\n",
      "\n",
      "        [[-0.6455,  0.8201,  0.2893,  ...,  0.3040,  0.0554, -0.7059],\n",
      "         [ 1.1776,  0.1091,  0.7856,  ..., -0.0476, -1.7465,  1.0767]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.up_proj.lora_B.weight': tensor([[[ 3.8701e-04, -5.4463e-04],\n",
      "         [ 6.4123e-04, -1.6376e-03],\n",
      "         [-6.6900e-04, -6.6089e-04],\n",
      "         ...,\n",
      "         [-4.6286e-04,  3.1143e-04],\n",
      "         [-9.4779e-04,  8.3398e-05],\n",
      "         [-1.0490e-03, -5.5351e-04]],\n",
      "\n",
      "        [[-1.2133e-04, -6.5387e-05],\n",
      "         [ 5.1688e-04, -2.5104e-04],\n",
      "         [-9.2727e-04,  3.2173e-04],\n",
      "         ...,\n",
      "         [-3.6089e-05,  6.9794e-04],\n",
      "         [ 5.1816e-05, -5.2927e-04],\n",
      "         [-3.6818e-04,  5.7882e-05]],\n",
      "\n",
      "        [[-2.3790e-04,  4.4345e-04],\n",
      "         [ 1.5050e-03, -7.1902e-04],\n",
      "         [-9.3074e-04, -3.2313e-04],\n",
      "         ...,\n",
      "         [-1.0872e-04,  3.5093e-04],\n",
      "         [-3.0839e-04, -3.6863e-04],\n",
      "         [-7.0436e-04, -6.8360e-04]],\n",
      "\n",
      "        [[ 3.9236e-04, -6.9101e-04],\n",
      "         [ 1.3710e-03, -6.6365e-04],\n",
      "         [-5.7928e-04, -1.4227e-03],\n",
      "         ...,\n",
      "         [-2.7686e-04,  1.7050e-04],\n",
      "         [-3.2287e-04, -3.2209e-05],\n",
      "         [-1.4352e-03, -2.4044e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.up_proj.lora_A.weight': tensor([[[-0.1380, -0.0727,  0.6078,  ..., -1.2063,  0.0279, -0.1539],\n",
      "         [ 1.7723, -0.0405, -0.1921,  ..., -0.2473, -0.9664,  0.8627]],\n",
      "\n",
      "        [[-0.1453,  0.6759, -0.4019,  ..., -0.6232,  0.1679, -0.6461],\n",
      "         [ 1.0028,  0.2058, -0.7745,  ..., -0.3863, -1.2726,  0.1272]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6467,  0.8121,  0.2817,  ...,  0.2923,  0.0384, -0.6882],\n",
      "         [ 1.1758,  0.1022,  0.7861,  ..., -0.0547, -1.7476,  1.0929]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.up_proj.lora_B.weight': tensor([[[ 4.0137e-04, -5.5697e-04],\n",
      "         [ 6.1465e-04, -1.6123e-03],\n",
      "         [-6.8159e-04, -6.5863e-04],\n",
      "         ...,\n",
      "         [-4.5538e-04,  3.3332e-04],\n",
      "         [-9.2181e-04,  7.4747e-05],\n",
      "         [-1.0356e-03, -5.0795e-04]],\n",
      "\n",
      "        [[-1.1202e-04, -5.8673e-05],\n",
      "         [ 5.3084e-04, -2.6128e-04],\n",
      "         [-9.4579e-04,  3.0152e-04],\n",
      "         ...,\n",
      "         [-3.1290e-05,  7.0252e-04],\n",
      "         [ 5.9755e-05, -5.2717e-04],\n",
      "         [-3.6776e-04,  5.4419e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5202e-04],\n",
      "         [-3.0835e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0656e-04, -6.9609e-04],\n",
      "         [ 1.3589e-03, -6.6321e-04],\n",
      "         [-5.8414e-04, -1.4276e-03],\n",
      "         ...,\n",
      "         [-2.7676e-04,  1.6791e-04],\n",
      "         [-3.2936e-04, -3.8065e-05],\n",
      "         [-1.4349e-03, -1.9268e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.up_proj.lora_A.weight': tensor([[[-0.1240, -0.0782,  0.5900,  ..., -1.2082,  0.0249, -0.1594],\n",
      "         [ 1.7672, -0.0391, -0.2005,  ..., -0.2534, -0.9640,  0.8592]],\n",
      "\n",
      "        [[-0.1400,  0.6689, -0.4126,  ..., -0.6316,  0.1673, -0.6456],\n",
      "         [ 1.0114,  0.2006, -0.7792,  ..., -0.3993, -1.2708,  0.1377]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6466,  0.8125,  0.2843,  ...,  0.2941,  0.0420, -0.6917],\n",
      "         [ 1.1774,  0.1035,  0.7865,  ..., -0.0521, -1.7466,  1.0897]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.up_proj.lora_B.weight': tensor([[[ 3.9272e-04, -5.5814e-04],\n",
      "         [ 5.9876e-04, -1.6015e-03],\n",
      "         [-6.8561e-04, -6.3536e-04],\n",
      "         ...,\n",
      "         [-4.5997e-04,  3.3729e-04],\n",
      "         [-9.0620e-04,  6.9985e-05],\n",
      "         [-1.0303e-03, -4.9792e-04]],\n",
      "\n",
      "        [[-1.1968e-04, -6.4703e-05],\n",
      "         [ 5.1632e-04, -2.5095e-04],\n",
      "         [-9.3090e-04,  3.1946e-04],\n",
      "         ...,\n",
      "         [-3.5369e-05,  6.9849e-04],\n",
      "         [ 5.4260e-05, -5.2800e-04],\n",
      "         [-3.6659e-04,  6.0743e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5203e-04],\n",
      "         [-3.0834e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0578e-04, -6.9473e-04],\n",
      "         [ 1.3619e-03, -6.6380e-04],\n",
      "         [-5.8362e-04, -1.4299e-03],\n",
      "         ...,\n",
      "         [-2.7739e-04,  1.6829e-04],\n",
      "         [-3.2861e-04, -3.7400e-05],\n",
      "         [-1.4373e-03, -1.9951e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.up_proj.lora_A.weight': tensor([[[-0.1044, -0.0948,  0.6124,  ..., -1.2015,  0.0528, -0.2002],\n",
      "         [ 1.7922, -0.0538, -0.1473,  ..., -0.2516, -0.9817,  0.8491]],\n",
      "\n",
      "        [[-0.1413,  0.6698, -0.4103,  ..., -0.6304,  0.1674, -0.6444],\n",
      "         [ 1.0090,  0.2015, -0.7799,  ..., -0.3957, -1.2695,  0.1335]],\n",
      "\n",
      "        [[-0.3511,  1.0478,  0.7116,  ...,  0.1521,  0.7785, -1.3382],\n",
      "         [ 1.1574,  0.1130,  0.3411,  ..., -0.0116, -1.8571,  0.1391]],\n",
      "\n",
      "        [[-0.6455,  0.8201,  0.2893,  ...,  0.3040,  0.0554, -0.7059],\n",
      "         [ 1.1776,  0.1091,  0.7856,  ..., -0.0476, -1.7465,  1.0767]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.up_proj.lora_B.weight': tensor([[[ 3.5592e-04, -5.3800e-04],\n",
      "         [ 6.3059e-04, -1.6269e-03],\n",
      "         [-6.5856e-04, -6.2210e-04],\n",
      "         ...,\n",
      "         [-4.7253e-04,  3.0530e-04],\n",
      "         [-9.6022e-04,  7.8877e-05],\n",
      "         [-1.0542e-03, -5.4515e-04]],\n",
      "\n",
      "        [[-1.1554e-04, -6.3755e-05],\n",
      "         [ 5.2043e-04, -2.5313e-04],\n",
      "         [-9.3583e-04,  3.1572e-04],\n",
      "         ...,\n",
      "         [-3.4391e-05,  7.0029e-04],\n",
      "         [ 5.5817e-05, -5.2867e-04],\n",
      "         [-3.6805e-04,  5.9000e-05]],\n",
      "\n",
      "        [[-2.4290e-04,  4.4108e-04],\n",
      "         [ 1.4968e-03, -7.1395e-04],\n",
      "         [-9.3050e-04, -3.1833e-04],\n",
      "         ...,\n",
      "         [-1.1285e-04,  3.4877e-04],\n",
      "         [-3.0742e-04, -3.6921e-04],\n",
      "         [-6.9840e-04, -6.7725e-04]],\n",
      "\n",
      "        [[ 3.9236e-04, -6.9101e-04],\n",
      "         [ 1.3710e-03, -6.6365e-04],\n",
      "         [-5.7928e-04, -1.4227e-03],\n",
      "         ...,\n",
      "         [-2.7686e-04,  1.7051e-04],\n",
      "         [-3.2287e-04, -3.2209e-05],\n",
      "         [-1.4352e-03, -2.4044e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.up_proj.lora_A.weight': tensor([[[-0.1363, -0.0731,  0.6045,  ..., -1.2069,  0.0237, -0.1505],\n",
      "         [ 1.7689, -0.0401, -0.1977,  ..., -0.2481, -0.9632,  0.8634]],\n",
      "\n",
      "        [[-0.1390,  0.6660, -0.4102,  ..., -0.6331,  0.1688, -0.6487],\n",
      "         [ 1.0155,  0.1984, -0.7773,  ..., -0.4004, -1.2725,  0.1395]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6467,  0.8121,  0.2817,  ...,  0.2923,  0.0384, -0.6881],\n",
      "         [ 1.1757,  0.1022,  0.7861,  ..., -0.0547, -1.7477,  1.0929]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.up_proj.lora_B.weight': tensor([[[ 4.0318e-04, -5.5828e-04],\n",
      "         [ 6.1101e-04, -1.6105e-03],\n",
      "         [-6.8457e-04, -6.5601e-04],\n",
      "         ...,\n",
      "         [-4.5661e-04,  3.3630e-04],\n",
      "         [-9.1457e-04,  7.3420e-05],\n",
      "         [-1.0326e-03, -5.0344e-04]],\n",
      "\n",
      "        [[-1.2133e-04, -6.5387e-05],\n",
      "         [ 5.1688e-04, -2.5104e-04],\n",
      "         [-9.2727e-04,  3.2173e-04],\n",
      "         ...,\n",
      "         [-3.6089e-05,  6.9794e-04],\n",
      "         [ 5.1816e-05, -5.2927e-04],\n",
      "         [-3.6818e-04,  5.7882e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5202e-04],\n",
      "         [-3.0835e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0654e-04, -6.9610e-04],\n",
      "         [ 1.3589e-03, -6.6318e-04],\n",
      "         [-5.8415e-04, -1.4275e-03],\n",
      "         ...,\n",
      "         [-2.7675e-04,  1.6794e-04],\n",
      "         [-3.2937e-04, -3.8081e-05],\n",
      "         [-1.4349e-03, -1.9261e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.up_proj.lora_A.weight': tensor([[[-0.1263, -0.0962,  0.6411,  ..., -1.2021,  0.0472, -0.1801],\n",
      "         [ 1.7827, -0.0502, -0.1424,  ..., -0.2385, -0.9673,  0.8566]],\n",
      "\n",
      "        [[-0.1391,  0.6663, -0.4083,  ..., -0.6324,  0.1675, -0.6487],\n",
      "         [ 1.0118,  0.2000, -0.7771,  ..., -0.3963, -1.2712,  0.1371]],\n",
      "\n",
      "        [[-0.3492,  1.0433,  0.7133,  ...,  0.1515,  0.7812, -1.3430],\n",
      "         [ 1.1610,  0.1114,  0.3408,  ..., -0.0093, -1.8531,  0.1365]],\n",
      "\n",
      "        [[-0.6454,  0.8197,  0.2897,  ...,  0.3040,  0.0558, -0.7064],\n",
      "         [ 1.1781,  0.1093,  0.7855,  ..., -0.0471, -1.7462,  1.0762]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.up_proj.lora_B.weight': tensor([[[ 3.8701e-04, -5.4463e-04],\n",
      "         [ 6.4123e-04, -1.6376e-03],\n",
      "         [-6.6900e-04, -6.6089e-04],\n",
      "         ...,\n",
      "         [-4.6286e-04,  3.1143e-04],\n",
      "         [-9.4779e-04,  8.3398e-05],\n",
      "         [-1.0490e-03, -5.5351e-04]],\n",
      "\n",
      "        [[-1.1880e-04, -6.4171e-05],\n",
      "         [ 5.1848e-04, -2.5422e-04],\n",
      "         [-9.3290e-04,  3.1733e-04],\n",
      "         ...,\n",
      "         [-3.4920e-05,  6.9957e-04],\n",
      "         [ 5.5072e-05, -5.2813e-04],\n",
      "         [-3.6990e-04,  5.6673e-05]],\n",
      "\n",
      "        [[-2.4147e-04,  4.4158e-04],\n",
      "         [ 1.4978e-03, -7.1766e-04],\n",
      "         [-9.2868e-04, -3.1840e-04],\n",
      "         ...,\n",
      "         [-1.1501e-04,  3.5235e-04],\n",
      "         [-3.0966e-04, -3.7047e-04],\n",
      "         [-7.0526e-04, -6.7771e-04]],\n",
      "\n",
      "        [[ 3.9250e-04, -6.9085e-04],\n",
      "         [ 1.3714e-03, -6.6379e-04],\n",
      "         [-5.7908e-04, -1.4237e-03],\n",
      "         ...,\n",
      "         [-2.7717e-04,  1.7040e-04],\n",
      "         [-3.2259e-04, -3.2209e-05],\n",
      "         [-1.4356e-03, -2.3992e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.up_proj.lora_A.weight': tensor([[[-0.1263, -0.0962,  0.6411,  ..., -1.2021,  0.0472, -0.1801],\n",
      "         [ 1.7827, -0.0502, -0.1424,  ..., -0.2385, -0.9673,  0.8566]],\n",
      "\n",
      "        [[-0.1401,  0.6690, -0.4127,  ..., -0.6317,  0.1673, -0.6455],\n",
      "         [ 1.0115,  0.2006, -0.7793,  ..., -0.3995, -1.2708,  0.1378]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6525,  0.8191,  0.2695,  ...,  0.2905,  0.0204, -0.6749],\n",
      "         [ 1.1531,  0.0946,  0.7970,  ..., -0.0678, -1.7575,  1.1108]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.up_proj.lora_B.weight': tensor([[[ 3.8700e-04, -5.4463e-04],\n",
      "         [ 6.4123e-04, -1.6376e-03],\n",
      "         [-6.6900e-04, -6.6089e-04],\n",
      "         ...,\n",
      "         [-4.6286e-04,  3.1143e-04],\n",
      "         [-9.4779e-04,  8.3398e-05],\n",
      "         [-1.0490e-03, -5.5351e-04]],\n",
      "\n",
      "        [[-1.1974e-04, -6.4738e-05],\n",
      "         [ 5.1623e-04, -2.5083e-04],\n",
      "         [-9.3076e-04,  3.1960e-04],\n",
      "         ...,\n",
      "         [-3.5392e-05,  6.9844e-04],\n",
      "         [ 5.4174e-05, -5.2801e-04],\n",
      "         [-3.6650e-04,  6.0851e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5203e-04],\n",
      "         [-3.0834e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0044e-04, -7.0627e-04],\n",
      "         [ 1.3485e-03, -6.6247e-04],\n",
      "         [-5.9129e-04, -1.4118e-03],\n",
      "         ...,\n",
      "         [-2.6285e-04,  1.7013e-04],\n",
      "         [-3.2759e-04, -3.2760e-05],\n",
      "         [-1.4137e-03, -2.2250e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.up_proj.lora_A.weight': tensor([[[-0.1303, -0.0762,  0.5973,  ..., -1.2076,  0.0241, -0.1547],\n",
      "         [ 1.7673, -0.0391, -0.1991,  ..., -0.2506, -0.9630,  0.8611]],\n",
      "\n",
      "        [[-0.1392,  0.6663, -0.4083,  ..., -0.6324,  0.1675, -0.6487],\n",
      "         [ 1.0118,  0.2000, -0.7771,  ..., -0.3962, -1.2712,  0.1371]],\n",
      "\n",
      "        [[-0.3525,  1.0459,  0.7151,  ...,  0.1530,  0.7821, -1.3395],\n",
      "         [ 1.1603,  0.1139,  0.3394,  ..., -0.0063, -1.8526,  0.1317]],\n",
      "\n",
      "        [[-0.6454,  0.8197,  0.2897,  ...,  0.3040,  0.0558, -0.7063],\n",
      "         [ 1.1781,  0.1093,  0.7855,  ..., -0.0472, -1.7462,  1.0763]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.up_proj.lora_B.weight': tensor([[[ 3.9875e-04, -5.5866e-04],\n",
      "         [ 6.0401e-04, -1.6053e-03],\n",
      "         [-6.8570e-04, -6.4585e-04],\n",
      "         ...,\n",
      "         [-4.5787e-04,  3.3750e-04],\n",
      "         [-9.0817e-04,  7.1286e-05],\n",
      "         [-1.0306e-03, -5.0037e-04]],\n",
      "\n",
      "        [[-1.1878e-04, -6.4157e-05],\n",
      "         [ 5.1850e-04, -2.5422e-04],\n",
      "         [-9.3294e-04,  3.1730e-04],\n",
      "         ...,\n",
      "         [-3.4908e-05,  6.9958e-04],\n",
      "         [ 5.5085e-05, -5.2813e-04],\n",
      "         [-3.6988e-04,  5.6689e-05]],\n",
      "\n",
      "        [[-2.3977e-04,  4.4308e-04],\n",
      "         [ 1.5032e-03, -7.1777e-04],\n",
      "         [-9.3181e-04, -3.2192e-04],\n",
      "         ...,\n",
      "         [-1.1176e-04,  3.5258e-04],\n",
      "         [-3.0789e-04, -3.6922e-04],\n",
      "         [-7.0422e-04, -6.8001e-04]],\n",
      "\n",
      "        [[ 3.9248e-04, -6.9086e-04],\n",
      "         [ 1.3714e-03, -6.6378e-04],\n",
      "         [-5.7910e-04, -1.4236e-03],\n",
      "         ...,\n",
      "         [-2.7714e-04,  1.7041e-04],\n",
      "         [-3.2262e-04, -3.2208e-05],\n",
      "         [-1.4356e-03, -2.3999e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.up_proj.lora_A.weight': tensor([[[-0.1271, -0.0719,  0.5847,  ..., -1.2079,  0.0300, -0.1632],\n",
      "         [ 1.7762, -0.0406, -0.2006,  ..., -0.2558, -0.9729,  0.8605]],\n",
      "\n",
      "        [[-0.1401,  0.6690, -0.4127,  ..., -0.6317,  0.1673, -0.6455],\n",
      "         [ 1.0115,  0.2006, -0.7793,  ..., -0.3995, -1.2708,  0.1378]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6526,  0.8191,  0.2695,  ...,  0.2905,  0.0203, -0.6748],\n",
      "         [ 1.1530,  0.0946,  0.7971,  ..., -0.0679, -1.7575,  1.1109]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.up_proj.lora_B.weight': tensor([[[ 3.8610e-04, -5.5592e-04],\n",
      "         [ 6.0367e-04, -1.6008e-03],\n",
      "         [-6.7787e-04, -6.3261e-04],\n",
      "         ...,\n",
      "         [-4.5982e-04,  3.3358e-04],\n",
      "         [-9.2151e-04,  6.9377e-05],\n",
      "         [-1.0365e-03, -4.9775e-04]],\n",
      "\n",
      "        [[-1.1974e-04, -6.4738e-05],\n",
      "         [ 5.1623e-04, -2.5082e-04],\n",
      "         [-9.3076e-04,  3.1961e-04],\n",
      "         ...,\n",
      "         [-3.5392e-05,  6.9844e-04],\n",
      "         [ 5.4173e-05, -5.2801e-04],\n",
      "         [-3.6650e-04,  6.0851e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5203e-04],\n",
      "         [-3.0834e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0045e-04, -7.0628e-04],\n",
      "         [ 1.3484e-03, -6.6248e-04],\n",
      "         [-5.9132e-04, -1.4118e-03],\n",
      "         ...,\n",
      "         [-2.6278e-04,  1.7011e-04],\n",
      "         [-3.2758e-04, -3.2718e-05],\n",
      "         [-1.4136e-03, -2.2282e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.up_proj.lora_A.weight': tensor([[[-0.1273, -0.0773,  0.5941,  ..., -1.2079,  0.0245, -0.1569],\n",
      "         [ 1.7673, -0.0391, -0.1996,  ..., -0.2520, -0.9635,  0.8602]],\n",
      "\n",
      "        [[-0.1414,  0.6698, -0.4103,  ..., -0.6305,  0.1674, -0.6444],\n",
      "         [ 1.0090,  0.2015, -0.7799,  ..., -0.3957, -1.2695,  0.1336]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6454,  0.8197,  0.2897,  ...,  0.3040,  0.0558, -0.7064],\n",
      "         [ 1.1781,  0.1093,  0.7855,  ..., -0.0471, -1.7462,  1.0762]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.up_proj.lora_B.weight': tensor([[[ 3.9591e-04, -5.5850e-04],\n",
      "         [ 6.0156e-04, -1.6035e-03],\n",
      "         [-6.8556e-04, -6.4100e-04],\n",
      "         ...,\n",
      "         [-4.5883e-04,  3.3739e-04],\n",
      "         [-9.0723e-04,  7.0745e-05],\n",
      "         [-1.0304e-03, -4.9931e-04]],\n",
      "\n",
      "        [[-1.1552e-04, -6.3772e-05],\n",
      "         [ 5.2041e-04, -2.5307e-04],\n",
      "         [-9.3582e-04,  3.1575e-04],\n",
      "         ...,\n",
      "         [-3.4374e-05,  7.0030e-04],\n",
      "         [ 5.5828e-05, -5.2869e-04],\n",
      "         [-3.6804e-04,  5.9029e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5202e-04],\n",
      "         [-3.0835e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 3.9250e-04, -6.9085e-04],\n",
      "         [ 1.3714e-03, -6.6379e-04],\n",
      "         [-5.7908e-04, -1.4237e-03],\n",
      "         ...,\n",
      "         [-2.7717e-04,  1.7040e-04],\n",
      "         [-3.2259e-04, -3.2209e-05],\n",
      "         [-1.4356e-03, -2.3992e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.up_proj.lora_A.weight': tensor([[[-0.1410, -0.0457,  0.5275,  ..., -1.2144,  0.0074, -0.1427],\n",
      "         [ 1.7670, -0.0322, -0.2616,  ..., -0.2646, -0.9821,  0.8660]],\n",
      "\n",
      "        [[-0.1390,  0.6660, -0.4102,  ..., -0.6331,  0.1688, -0.6487],\n",
      "         [ 1.0155,  0.1984, -0.7773,  ..., -0.4004, -1.2725,  0.1395]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6526,  0.8191,  0.2695,  ...,  0.2905,  0.0203, -0.6748],\n",
      "         [ 1.1530,  0.0946,  0.7971,  ..., -0.0679, -1.7575,  1.1109]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.up_proj.lora_B.weight': tensor([[[ 4.0173e-04, -5.6615e-04],\n",
      "         [ 5.7214e-04, -1.5650e-03],\n",
      "         [-6.8956e-04, -6.1776e-04],\n",
      "         ...,\n",
      "         [-4.5176e-04,  3.5916e-04],\n",
      "         [-8.9099e-04,  4.5098e-05],\n",
      "         [-1.0266e-03, -4.4132e-04]],\n",
      "\n",
      "        [[-1.2133e-04, -6.5381e-05],\n",
      "         [ 5.1689e-04, -2.5105e-04],\n",
      "         [-9.2729e-04,  3.2171e-04],\n",
      "         ...,\n",
      "         [-3.6085e-05,  6.9794e-04],\n",
      "         [ 5.1825e-05, -5.2926e-04],\n",
      "         [-3.6819e-04,  5.7874e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5202e-04],\n",
      "         [-3.0835e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0045e-04, -7.0628e-04],\n",
      "         [ 1.3484e-03, -6.6248e-04],\n",
      "         [-5.9132e-04, -1.4118e-03],\n",
      "         ...,\n",
      "         [-2.6278e-04,  1.7011e-04],\n",
      "         [-3.2758e-04, -3.2718e-05],\n",
      "         [-1.4136e-03, -2.2282e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.up_proj.lora_A.weight': tensor([[[-0.1044, -0.0948,  0.6124,  ..., -1.2015,  0.0528, -0.2002],\n",
      "         [ 1.7922, -0.0538, -0.1473,  ..., -0.2516, -0.9817,  0.8491]],\n",
      "\n",
      "        [[-0.1413,  0.6698, -0.4103,  ..., -0.6304,  0.1674, -0.6444],\n",
      "         [ 1.0090,  0.2015, -0.7799,  ..., -0.3957, -1.2695,  0.1335]],\n",
      "\n",
      "        [[-0.3535,  1.0434,  0.7202,  ...,  0.1527,  0.7863, -1.3422],\n",
      "         [ 1.1620,  0.1137,  0.3432,  ..., -0.0049, -1.8524,  0.1293]],\n",
      "\n",
      "        [[-0.6455,  0.8201,  0.2893,  ...,  0.3040,  0.0554, -0.7059],\n",
      "         [ 1.1776,  0.1091,  0.7856,  ..., -0.0476, -1.7465,  1.0767]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.up_proj.lora_B.weight': tensor([[[ 3.5590e-04, -5.3800e-04],\n",
      "         [ 6.3057e-04, -1.6268e-03],\n",
      "         [-6.5856e-04, -6.2207e-04],\n",
      "         ...,\n",
      "         [-4.7253e-04,  3.0530e-04],\n",
      "         [-9.6021e-04,  7.8870e-05],\n",
      "         [-1.0541e-03, -5.4514e-04]],\n",
      "\n",
      "        [[-1.1554e-04, -6.3756e-05],\n",
      "         [ 5.2043e-04, -2.5312e-04],\n",
      "         [-9.3583e-04,  3.1572e-04],\n",
      "         ...,\n",
      "         [-3.4390e-05,  7.0029e-04],\n",
      "         [ 5.5818e-05, -5.2868e-04],\n",
      "         [-3.6804e-04,  5.9002e-05]],\n",
      "\n",
      "        [[-2.3771e-04,  4.4357e-04],\n",
      "         [ 1.5052e-03, -7.1981e-04],\n",
      "         [-9.3041e-04, -3.2343e-04],\n",
      "         ...,\n",
      "         [-1.0924e-04,  3.5201e-04],\n",
      "         [-3.0838e-04, -3.6853e-04],\n",
      "         [-7.0596e-04, -6.8356e-04]],\n",
      "\n",
      "        [[ 3.9236e-04, -6.9101e-04],\n",
      "         [ 1.3710e-03, -6.6365e-04],\n",
      "         [-5.7928e-04, -1.4227e-03],\n",
      "         ...,\n",
      "         [-2.7686e-04,  1.7050e-04],\n",
      "         [-3.2287e-04, -3.2209e-05],\n",
      "         [-1.4352e-03, -2.4044e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.up_proj.lora_A.weight': tensor([[[-0.1045, -0.0949,  0.6128,  ..., -1.2014,  0.0529, -0.2000],\n",
      "         [ 1.7921, -0.0537, -0.1471,  ..., -0.2516, -0.9815,  0.8492]],\n",
      "\n",
      "        [[-0.1401,  0.6690, -0.4127,  ..., -0.6317,  0.1673, -0.6455],\n",
      "         [ 1.0115,  0.2006, -0.7793,  ..., -0.3995, -1.2708,  0.1378]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6526,  0.8191,  0.2695,  ...,  0.2905,  0.0203, -0.6748],\n",
      "         [ 1.1530,  0.0946,  0.7971,  ..., -0.0679, -1.7575,  1.1109]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.up_proj.lora_B.weight': tensor([[[ 3.5620e-04, -5.3813e-04],\n",
      "         [ 6.3065e-04, -1.6270e-03],\n",
      "         [-6.5865e-04, -6.2239e-04],\n",
      "         ...,\n",
      "         [-4.7243e-04,  3.0533e-04],\n",
      "         [-9.6009e-04,  7.9033e-05],\n",
      "         [-1.0540e-03, -5.4525e-04]],\n",
      "\n",
      "        [[-1.1974e-04, -6.4738e-05],\n",
      "         [ 5.1623e-04, -2.5082e-04],\n",
      "         [-9.3076e-04,  3.1961e-04],\n",
      "         ...,\n",
      "         [-3.5392e-05,  6.9844e-04],\n",
      "         [ 5.4173e-05, -5.2801e-04],\n",
      "         [-3.6650e-04,  6.0851e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5203e-04],\n",
      "         [-3.0834e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0045e-04, -7.0628e-04],\n",
      "         [ 1.3484e-03, -6.6248e-04],\n",
      "         [-5.9132e-04, -1.4118e-03],\n",
      "         ...,\n",
      "         [-2.6278e-04,  1.7011e-04],\n",
      "         [-3.2758e-04, -3.2718e-05],\n",
      "         [-1.4136e-03, -2.2282e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.up_proj.lora_A.weight': tensor([[[-0.1238, -0.0783,  0.5897,  ..., -1.2083,  0.0250, -0.1596],\n",
      "         [ 1.7672, -0.0391, -0.2006,  ..., -0.2535, -0.9640,  0.8592]],\n",
      "\n",
      "        [[-0.1390,  0.6660, -0.4102,  ..., -0.6331,  0.1688, -0.6487],\n",
      "         [ 1.0155,  0.1984, -0.7773,  ..., -0.4004, -1.2725,  0.1395]],\n",
      "\n",
      "        [[-0.3521,  1.0455,  0.7152,  ...,  0.1525,  0.7819, -1.3394],\n",
      "         [ 1.1602,  0.1137,  0.3396,  ..., -0.0066, -1.8528,  0.1321]],\n",
      "\n",
      "        [[-0.6467,  0.8120,  0.2819,  ...,  0.2924,  0.0385, -0.6883],\n",
      "         [ 1.1759,  0.1023,  0.7862,  ..., -0.0546, -1.7476,  1.0928]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.up_proj.lora_B.weight': tensor([[[ 3.9251e-04, -5.5810e-04],\n",
      "         [ 5.9859e-04, -1.6014e-03],\n",
      "         [-6.8563e-04, -6.3499e-04],\n",
      "         ...,\n",
      "         [-4.6007e-04,  3.3729e-04],\n",
      "         [-9.0614e-04,  6.9930e-05],\n",
      "         [-1.0303e-03, -4.9783e-04]],\n",
      "\n",
      "        [[-1.2133e-04, -6.5387e-05],\n",
      "         [ 5.1688e-04, -2.5104e-04],\n",
      "         [-9.2727e-04,  3.2173e-04],\n",
      "         ...,\n",
      "         [-3.6089e-05,  6.9794e-04],\n",
      "         [ 5.1816e-05, -5.2927e-04],\n",
      "         [-3.6818e-04,  5.7882e-05]],\n",
      "\n",
      "        [[-2.3976e-04,  4.4326e-04],\n",
      "         [ 1.5026e-03, -7.1802e-04],\n",
      "         [-9.3200e-04, -3.2165e-04],\n",
      "         ...,\n",
      "         [-1.1187e-04,  3.5202e-04],\n",
      "         [-3.0835e-04, -3.6921e-04],\n",
      "         [-7.0408e-04, -6.8003e-04]],\n",
      "\n",
      "        [[ 4.0662e-04, -6.9606e-04],\n",
      "         [ 1.3590e-03, -6.6327e-04],\n",
      "         [-5.8413e-04, -1.4278e-03],\n",
      "         ...,\n",
      "         [-2.7679e-04,  1.6787e-04],\n",
      "         [-3.2934e-04, -3.8037e-05],\n",
      "         [-1.4351e-03, -1.9281e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.up_proj.lora_A.weight': tensor([[[-0.1263, -0.0962,  0.6411,  ..., -1.2021,  0.0472, -0.1801],\n",
      "         [ 1.7827, -0.0502, -0.1424,  ..., -0.2385, -0.9673,  0.8566]],\n",
      "\n",
      "        [[-0.1413,  0.6698, -0.4103,  ..., -0.6304,  0.1674, -0.6444],\n",
      "         [ 1.0090,  0.2015, -0.7799,  ..., -0.3957, -1.2695,  0.1335]],\n",
      "\n",
      "        [[-0.3542,  1.0442,  0.7200,  ...,  0.1529,  0.7862, -1.3412],\n",
      "         [ 1.1615,  0.1140,  0.3435,  ..., -0.0056, -1.8534,  0.1300]],\n",
      "\n",
      "        [[-0.6466,  0.8125,  0.2843,  ...,  0.2941,  0.0420, -0.6917],\n",
      "         [ 1.1774,  0.1035,  0.7865,  ..., -0.0521, -1.7466,  1.0896]]],\n",
      "       device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.up_proj.lora_B.weight': tensor([[[ 3.8701e-04, -5.4463e-04],\n",
      "         [ 6.4123e-04, -1.6376e-03],\n",
      "         [-6.6900e-04, -6.6089e-04],\n",
      "         ...,\n",
      "         [-4.6286e-04,  3.1143e-04],\n",
      "         [-9.4779e-04,  8.3398e-05],\n",
      "         [-1.0490e-03, -5.5351e-04]],\n",
      "\n",
      "        [[-1.1554e-04, -6.3755e-05],\n",
      "         [ 5.2043e-04, -2.5313e-04],\n",
      "         [-9.3583e-04,  3.1572e-04],\n",
      "         ...,\n",
      "         [-3.4391e-05,  7.0029e-04],\n",
      "         [ 5.5817e-05, -5.2867e-04],\n",
      "         [-3.6805e-04,  5.9000e-05]],\n",
      "\n",
      "        [[-2.3788e-04,  4.4346e-04],\n",
      "         [ 1.5050e-03, -7.1911e-04],\n",
      "         [-9.3071e-04, -3.2314e-04],\n",
      "         ...,\n",
      "         [-1.0878e-04,  3.5106e-04],\n",
      "         [-3.0839e-04, -3.6863e-04],\n",
      "         [-7.0453e-04, -6.8358e-04]],\n",
      "\n",
      "        [[ 4.0578e-04, -6.9473e-04],\n",
      "         [ 1.3619e-03, -6.6380e-04],\n",
      "         [-5.8362e-04, -1.4299e-03],\n",
      "         ...,\n",
      "         [-2.7739e-04,  1.6829e-04],\n",
      "         [-3.2861e-04, -3.7399e-05],\n",
      "         [-1.4373e-03, -1.9952e-05]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4006e+00, -2.7491e-01, -1.3677e+00,  ...,  1.0448e+00,\n",
      "          -4.1487e-01, -9.4510e-02],\n",
      "         [ 9.2754e-01, -9.8890e-02,  3.1993e-01,  ...,  2.5074e+00,\n",
      "          -5.5054e-01,  9.6488e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1476e-03],\n",
      "         [-7.6931e-04,  2.6644e-04],\n",
      "         [ 1.2640e-03, -2.0613e-03],\n",
      "         ...,\n",
      "         [-1.0646e-04, -4.0389e-04],\n",
      "         [-6.1508e-04, -9.0970e-04],\n",
      "         [ 6.9759e-04, -3.8265e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7521e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1406e-01, -9.4287e-02],\n",
      "         [ 9.2760e-01, -9.8776e-02,  3.1982e-01,  ...,  2.5070e+00,\n",
      "          -5.5090e-01,  9.6439e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6973e-04,  2.6698e-04],\n",
      "         [ 1.2644e-03, -2.0610e-03],\n",
      "         ...,\n",
      "         [-1.0668e-04, -4.0386e-04],\n",
      "         [-6.1526e-04, -9.0954e-04],\n",
      "         [ 6.9710e-04, -3.7762e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9436e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4012e+00, -2.7514e-01, -1.3673e+00,  ...,  1.0451e+00,\n",
      "          -4.1425e-01, -9.4338e-02],\n",
      "         [ 9.2759e-01, -9.8801e-02,  3.1984e-01,  ...,  2.5071e+00,\n",
      "          -5.5081e-01,  9.6450e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1474e-03],\n",
      "         [-7.6963e-04,  2.6686e-04],\n",
      "         [ 1.2643e-03, -2.0611e-03],\n",
      "         ...,\n",
      "         [-1.0663e-04, -4.0387e-04],\n",
      "         [-6.1522e-04, -9.0958e-04],\n",
      "         [ 6.9722e-04, -3.7883e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9436e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4008e+00, -2.7501e-01, -1.3675e+00,  ...,  1.0449e+00,\n",
      "          -4.1460e-01, -9.4432e-02],\n",
      "         [ 9.2756e-01, -9.8849e-02,  3.1989e-01,  ...,  2.5073e+00,\n",
      "          -5.5066e-01,  9.6472e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6945e-04,  2.6662e-04],\n",
      "         [ 1.2641e-03, -2.0612e-03],\n",
      "         ...,\n",
      "         [-1.0654e-04, -4.0388e-04],\n",
      "         [-6.1515e-04, -9.0965e-04],\n",
      "         [ 6.9744e-04, -3.8100e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7522e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1403e-01, -9.4282e-02],\n",
      "         [ 9.2760e-01, -9.8773e-02,  3.1981e-01,  ...,  2.5069e+00,\n",
      "          -5.5091e-01,  9.6437e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6975e-04,  2.6700e-04],\n",
      "         [ 1.2644e-03, -2.0610e-03],\n",
      "         ...,\n",
      "         [-1.0668e-04, -4.0386e-04],\n",
      "         [-6.1526e-04, -9.0953e-04],\n",
      "         [ 6.9709e-04, -3.7748e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5811e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7520e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1408e-01, -9.4292e-02],\n",
      "         [ 9.2760e-01, -9.8778e-02,  3.1982e-01,  ...,  2.5070e+00,\n",
      "          -5.5089e-01,  9.6440e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6972e-04,  2.6697e-04],\n",
      "         [ 1.2644e-03, -2.0611e-03],\n",
      "         ...,\n",
      "         [-1.0667e-04, -4.0387e-04],\n",
      "         [-6.1526e-04, -9.0954e-04],\n",
      "         [ 6.9711e-04, -3.7774e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4006e+00, -2.7494e-01, -1.3677e+00,  ...,  1.0448e+00,\n",
      "          -4.1480e-01, -9.4489e-02],\n",
      "         [ 9.2755e-01, -9.8879e-02,  3.1992e-01,  ...,  2.5074e+00,\n",
      "          -5.5058e-01,  9.6483e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6935e-04,  2.6649e-04],\n",
      "         [ 1.2640e-03, -2.0613e-03],\n",
      "         ...,\n",
      "         [-1.0648e-04, -4.0389e-04],\n",
      "         [-6.1510e-04, -9.0968e-04],\n",
      "         [ 6.9755e-04, -3.8220e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4011e+00, -2.7509e-01, -1.3674e+00,  ...,  1.0450e+00,\n",
      "          -4.1437e-01, -9.4369e-02],\n",
      "         [ 9.2758e-01, -9.8816e-02,  3.1986e-01,  ...,  2.5071e+00,\n",
      "          -5.5076e-01,  9.6458e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1474e-03],\n",
      "         [-7.6957e-04,  2.6677e-04],\n",
      "         [ 1.2642e-03, -2.0612e-03],\n",
      "         ...,\n",
      "         [-1.0660e-04, -4.0387e-04],\n",
      "         [-6.1520e-04, -9.0960e-04],\n",
      "         [ 6.9730e-04, -3.7958e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7520e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1407e-01, -9.4291e-02],\n",
      "         [ 9.2760e-01, -9.8777e-02,  3.1982e-01,  ...,  2.5070e+00,\n",
      "          -5.5089e-01,  9.6440e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6973e-04,  2.6697e-04],\n",
      "         [ 1.2644e-03, -2.0611e-03],\n",
      "         ...,\n",
      "         [-1.0667e-04, -4.0387e-04],\n",
      "         [-6.1526e-04, -9.0954e-04],\n",
      "         [ 6.9711e-04, -3.7772e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4007e+00, -2.7496e-01, -1.3676e+00,  ...,  1.0448e+00,\n",
      "          -4.1474e-01, -9.4470e-02],\n",
      "         [ 9.2755e-01, -9.8869e-02,  3.1991e-01,  ...,  2.5073e+00,\n",
      "          -5.5060e-01,  9.6480e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6938e-04,  2.6653e-04],\n",
      "         [ 1.2641e-03, -2.0613e-03],\n",
      "         ...,\n",
      "         [-1.0650e-04, -4.0389e-04],\n",
      "         [-6.1512e-04, -9.0967e-04],\n",
      "         [ 6.9752e-04, -3.8183e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4009e+00, -2.7503e-01, -1.3675e+00,  ...,  1.0449e+00,\n",
      "          -4.1454e-01, -9.4415e-02],\n",
      "         [ 9.2757e-01, -9.8840e-02,  3.1989e-01,  ...,  2.5072e+00,\n",
      "          -5.5069e-01,  9.6468e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6948e-04,  2.6666e-04],\n",
      "         [ 1.2641e-03, -2.0612e-03],\n",
      "         ...,\n",
      "         [-1.0655e-04, -4.0388e-04],\n",
      "         [-6.1516e-04, -9.0963e-04],\n",
      "         [ 6.9740e-04, -3.8062e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7521e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1404e-01, -9.4283e-02],\n",
      "         [ 9.2760e-01, -9.8773e-02,  3.1981e-01,  ...,  2.5069e+00,\n",
      "          -5.5090e-01,  9.6438e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6974e-04,  2.6699e-04],\n",
      "         [ 1.2644e-03, -2.0610e-03],\n",
      "         ...,\n",
      "         [-1.0668e-04, -4.0386e-04],\n",
      "         [-6.1526e-04, -9.0954e-04],\n",
      "         [ 6.9709e-04, -3.7752e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4005e+00, -2.7489e-01, -1.3678e+00,  ...,  1.0447e+00,\n",
      "          -4.1494e-01, -9.4525e-02],\n",
      "         [ 9.2753e-01, -9.8898e-02,  3.1994e-01,  ...,  2.5074e+00,\n",
      "          -5.5052e-01,  9.6492e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1476e-03],\n",
      "         [-7.6927e-04,  2.6640e-04],\n",
      "         [ 1.2640e-03, -2.0613e-03],\n",
      "         ...,\n",
      "         [-1.0645e-04, -4.0389e-04],\n",
      "         [-6.1507e-04, -9.0971e-04],\n",
      "         [ 6.9764e-04, -3.8304e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9436e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4007e+00, -2.7494e-01, -1.3677e+00,  ...,  1.0448e+00,\n",
      "          -4.1479e-01, -9.4483e-02],\n",
      "         [ 9.2755e-01, -9.8876e-02,  3.1992e-01,  ...,  2.5074e+00,\n",
      "          -5.5058e-01,  9.6483e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6935e-04,  2.6650e-04],\n",
      "         [ 1.2640e-03, -2.0613e-03],\n",
      "         ...,\n",
      "         [-1.0649e-04, -4.0389e-04],\n",
      "         [-6.1510e-04, -9.0968e-04],\n",
      "         [ 6.9755e-04, -3.8213e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4008e+00, -2.7500e-01, -1.3676e+00,  ...,  1.0449e+00,\n",
      "          -4.1462e-01, -9.4436e-02],\n",
      "         [ 9.2756e-01, -9.8851e-02,  3.1990e-01,  ...,  2.5073e+00,\n",
      "          -5.5066e-01,  9.6473e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1475e-03],\n",
      "         [-7.6944e-04,  2.6661e-04],\n",
      "         [ 1.2641e-03, -2.0612e-03],\n",
      "         ...,\n",
      "         [-1.0653e-04, -4.0388e-04],\n",
      "         [-6.1514e-04, -9.0965e-04],\n",
      "         [ 6.9745e-04, -3.8109e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight': tensor([[[-1.1302e+00, -1.0223e-03, -1.6732e+00,  ...,  9.5604e-01,\n",
      "          -1.0612e+00, -1.2337e+00],\n",
      "         [ 5.2256e-01, -7.7604e-01,  5.0538e-01,  ...,  2.3368e+00,\n",
      "           3.5192e-01,  1.5209e+00]],\n",
      "\n",
      "        [[-2.4014e+00, -2.7519e-01, -1.3672e+00,  ...,  1.0452e+00,\n",
      "          -4.1410e-01, -9.4298e-02],\n",
      "         [ 9.2760e-01, -9.8781e-02,  3.1982e-01,  ...,  2.5070e+00,\n",
      "          -5.5088e-01,  9.6441e-01]],\n",
      "\n",
      "        [[-1.0328e+00,  1.2856e-01, -1.1987e+00,  ...,  4.0749e-01,\n",
      "          -9.0756e-01, -9.2666e-01],\n",
      "         [ 1.4458e+00, -9.5100e-01,  4.5239e-01,  ...,  1.6521e+00,\n",
      "           6.3731e-01,  1.2161e+00]],\n",
      "\n",
      "        [[-6.5026e-01, -3.1267e-01, -9.2843e-01,  ...,  1.4915e-01,\n",
      "          -1.2105e+00, -9.6373e-01],\n",
      "         [ 6.6251e-01, -1.2127e+00,  5.7170e-01,  ...,  2.0034e+00,\n",
      "           6.4855e-01,  1.4626e+00]]], device='cuda:5', grad_fn=<CatBackward0>), 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.weight': tensor([[[-3.6533e-06, -1.2555e-03],\n",
      "         [-2.0588e-04, -4.9323e-04],\n",
      "         [ 4.8851e-04, -1.7032e-03],\n",
      "         ...,\n",
      "         [ 3.0845e-04, -3.7466e-04],\n",
      "         [-4.7502e-04, -6.9446e-04],\n",
      "         [ 5.0432e-04, -4.1323e-04]],\n",
      "\n",
      "        [[-3.1351e-04, -1.1473e-03],\n",
      "         [-7.6971e-04,  2.6696e-04],\n",
      "         [ 1.2644e-03, -2.0611e-03],\n",
      "         ...,\n",
      "         [-1.0667e-04, -4.0387e-04],\n",
      "         [-6.1525e-04, -9.0955e-04],\n",
      "         [ 6.9712e-04, -3.7787e-05]],\n",
      "\n",
      "        [[-4.4992e-05, -5.3002e-04],\n",
      "         [-7.3772e-04, -2.9610e-04],\n",
      "         [ 7.6826e-04, -1.9600e-03],\n",
      "         ...,\n",
      "         [ 5.9435e-04, -2.9895e-04],\n",
      "         [-8.8701e-04, -7.0523e-04],\n",
      "         [ 7.5810e-04, -2.9489e-04]],\n",
      "\n",
      "        [[ 4.0528e-04, -9.2459e-04],\n",
      "         [-4.5095e-04, -1.4019e-04],\n",
      "         [ 7.9398e-04, -1.5709e-03],\n",
      "         ...,\n",
      "         [ 6.7019e-04,  7.0946e-05],\n",
      "         [-3.4055e-04, -5.7204e-04],\n",
      "         [ 6.3187e-04, -1.5036e-04]]], device='cuda:5', grad_fn=<CatBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from train_my_dyprag import *\n",
    "llm_model_path = \"./models/Llama-3.2-1B-Instruct-Doc_mask\"\n",
    "embedding_model_path = \"./models/long-t5-tglobal-base\"\n",
    "device = \"cuda:5\"\n",
    "translator_path = \"./models/Llama-3.2-1B-Instruct-Doc_mask-longt5_capt_4/translator_step_70000.safetensors\"\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_path,device_map=device)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, device_map=device)\n",
    "translator = CrossAttentionParameterTranslator(\n",
    "            embedding_model=embedding_model,\n",
    "            llm_model=llm_model,\n",
    "            lora_rank=2,\n",
    "            projector_hidden_dim=2560,\n",
    "            attn_heads=8,\n",
    "            attn_ff_dim=1024,\n",
    "            cross_layers=4,\n",
    "            encoder_layers=4,\n",
    "        )\n",
    "translator.to(device)\n",
    "translator.load_state_dict(load_file(translator_path, device=device), strict=False)\n",
    "translator.eval()\n",
    "x=torch.randn(4, 16, 768).to(device)\n",
    "attention_mask = torch.ones((4, 16), dtype=torch.float32, device=device)\n",
    "y=translator(x, attention_mask)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ba552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from imports import *\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text with lowercasing, removing articles, and punctuation.\"\"\"\n",
    "\n",
    "    # 定义一个函数，用于移除文本中的冠词\n",
    "    def remove_articles(text: str) -> str:\n",
    "        # 使用正则表达式，将文本中的冠词替换为空格\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    # 定义一个函数，用于去除字符串中的多余空格\n",
    "    def white_space_fix(text: str) -> str:\n",
    "        # 使用split()方法将字符串按空格分割成一个列表\n",
    "        # 使用join()方法将列表中的元素用空格连接成一个字符串\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # 定义一个函数，用于移除字符串中的标点符号\n",
    "    def remove_punc(text: str) -> str:\n",
    "        # 定义一个集合，包含所有标点符号\n",
    "        exclude = set(string.punctuation)\n",
    "        # 返回一个新的字符串，其中不包含标点符号\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # 定义一个函数，将输入的字符串转换为小写\n",
    "    def lower(text: str) -> str:\n",
    "        # 返回转换后的小写字符串\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(text))))\n",
    "\n",
    "def calc_unigram_f1(text: str, answers: list[str], field: str = \"f1\") -> float:\n",
    "    norm_pred = normalize_text(text).split()\n",
    "    norm_answers = [normalize_text(ans).split() for ans in answers]\n",
    "\n",
    "    score_list = []\n",
    "    for norm_ans in norm_answers:\n",
    "        common = Counter(norm_pred) & Counter(norm_ans)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            score_list.append(0.0)\n",
    "            continue\n",
    "        p = num_same / len(norm_pred)\n",
    "        r = num_same / len(norm_ans)\n",
    "        if field == \"precision\":\n",
    "            score_list.append(p)\n",
    "        elif field == \"recall\":\n",
    "            score_list.append(r)\n",
    "        elif field == \"f1\":\n",
    "            f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
    "            score_list.append(f1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown field: {field}\")\n",
    "    return max(score_list)\n",
    "def calc_subspan_em(text: str, answers: list[str]) -> float:\n",
    "    \"\"\"Calculate exact match score between the text and reference answers.\"\"\"\n",
    "    norm_pred = normalize_text(text)\n",
    "    norm_answers = [normalize_text(ans) for ans in answers]\n",
    "    em = [1.0 if norm_ans in norm_pred else 0.0 for norm_ans in norm_answers]\n",
    "    return max(em)\n",
    "# 测试 F1 指标\n",
    "assert round(calc_unigram_f1(\"The quick brown fox\", [\"quick brown fox\"]), 2) == 1.0\n",
    "assert round(calc_unigram_f1(\"The quick brown fox\", [\"slow green turtle\"]), 2) == 0.0\n",
    "assert round(calc_unigram_f1(\"Quick brown fox\", [\"quick fox\"]), 2) == 0.8  # precision = 2/3, recall = 2/2, F1 ≈ 0.8\n",
    "\n",
    "# 测试 Subspan EM\n",
    "assert calc_subspan_em(\"The quick brown fox\", [\"quick brown\"]) == 1.0\n",
    "assert calc_subspan_em(\"The quick brown fox\", [\"slow turtle\"]) == 0.0\n",
    "assert calc_subspan_em(\"Brown fox jumps\", [\"fox jumps\"]) == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f29a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compositional : 5236\n",
      "comparison : 3040\n",
      "inference : 1549\n",
      "bridge_comparison : 2751\n",
      "comparison : 1487\n",
      "bridge : 5918\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "input_json_paths=[\n",
    "        \"./data/2wikimultihopqa/dev.json\",\n",
    "        # \"./data_aug_projector/complexwebquestions/llama3.2-1b-instruct/total.json\",\n",
    "        \"./data/hotpotqa/hotpot_dev_distractor_v1.json\",\n",
    "        # \"./data/ragtruth/source_info.jsonl\",\n",
    "    ]\n",
    "# compositional : 5236\n",
    "# comparison : 3040\n",
    "# inference : 1549\n",
    "# bridge_comparison : 2751\n",
    "\n",
    "for input_json_path in input_json_paths:\n",
    "    if \"2wikimultihopqa\" in input_json_path or \"hotpotqa\" in input_json_path:\n",
    "        with open(input_json_path) as f:\n",
    "            data = json.load(f)\n",
    "        splits = {}\n",
    "        for item in data:\n",
    "            if item['type'] not in splits:\n",
    "                splits[item['type']] = []\n",
    "            passages = []\n",
    "            for passage in item['context']:\n",
    "                passages.append(passage[0]+\":\\n\"+\"\\n\".join(passage[1]))\n",
    "            splits[item['type']].append(\n",
    "                {\n",
    "                    \"_id\": item['_id'],\n",
    "                    \"question\": item['question'],\n",
    "                    \"answer\":  [item['answer']],\n",
    "                    \"passages\": passages,\n",
    "                }\n",
    "            )\n",
    "        for split_name, split_data in splits.items():\n",
    "            print(split_name,\":\",len(split_data))\n",
    "            save_path = \"./data_devall/\"+Path(input_json_path).parts[1]+f\"/{split_name}.json\"\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            with open(\"./data_devall/\"+Path(input_json_path).parts[1]+f\"/{split_name}.json\", \"w\") as f:\n",
    "                json.dump(split_data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e72cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=46.5544, special_vector=tensor([1.3469, 1.1061, 0.5712, 0.5527])\n",
      "Step 1: loss=41.2559, special_vector=tensor([1.4467, 1.2059, 0.6710, 0.6525])\n",
      "Step 2: loss=36.2872, special_vector=tensor([1.5461, 1.3054, 0.7704, 0.7519])\n",
      "Step 3: loss=31.6533, special_vector=tensor([1.6450, 1.4043, 0.8694, 0.8509])\n",
      "Step 4: loss=27.3579, special_vector=tensor([1.7432, 1.5025, 0.9676, 0.9491])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模拟一个 embedding table\n",
    "embedding = nn.Embedding(10, 4)\n",
    "for p in embedding.parameters():\n",
    "    p.requires_grad = False  # 冻结整个 embedding matrix\n",
    "\n",
    "# 我们只训练这一个特殊 token 的向量（假设 index 为 5）\n",
    "special_vector = nn.Parameter(embedding.weight[5].clone().detach())\n",
    "optimizer = torch.optim.Adam([special_vector], lr=0.1)\n",
    "\n",
    "# 输入 id，假设只有一个 token，id 为 5\n",
    "input_ids = torch.tensor([5])\n",
    "\n",
    "# forward 时替换 index 5 的 embedding 向量\n",
    "def custom_forward(input_ids):\n",
    "    embedded = embedding(input_ids)  # shape: [1, 4]\n",
    "    embedded[0] = special_vector     # 替换掉 index 5 的向量\n",
    "    output = embedded.sum()\n",
    "    return output\n",
    "\n",
    "# 训练循环\n",
    "for step in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = custom_forward(input_ids)\n",
    "    loss = (output - 10).pow(2)  # 想让输出接近 10\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}: loss={loss.item():.4f}, special_vector={special_vector.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "print(dataset_test)\n",
    "datas = []\n",
    "for i in range(100):\n",
    "    article = dataset_test[i]['article']\n",
    "    highlights = dataset_test[i]['highlights']\n",
    "    datas.append({\n",
    "        'article': article,\n",
    "        'highlights': highlights,\n",
    "    }\n",
    "    )\n",
    "save_path = \"./data_dev_inference/cnn_dailymail/test_100.json\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(datas, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
